\documentclass[a4paper,12pt]{article}

\input{modele_documents.tex}

\usepackage[toc,page]{appendix}
\renewcommand\appendixtocname{Annexes}
\newcommand{\supp}{\text{supp }}
\newcommand{\prox}{\text{Prox}}

\setlength{\parindent}{0pt}
\newcommand\titre{Transport Optimal}
\newcommand\auteur{Timothée \textsc{Schmoderer}}
\newcommand\dateDoc{2017/2018}
\newcommand\chapitre{Chapitre 2}
\newcommand\cours{PFE}
\usepackage{enumitem}
\everymath{\displaystyle}

\title{\titre }
\author{\auteur}
\date{\dateDoc}

\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}

\lhead{\cours}
\chead{}
\rhead{\currentname}
\lfoot{\titre}
\cfoot{}
\rfoot{Page \thepage\ /\ \pageref*{LastPage}}  


\lstset{
language=Matlab,
}

\hypersetup {
 pdftitle={\titre},    % title
    pdfauthor={\auteur},     % author
    pdfsubject={\cours},   % subject of the document
    pdfkeywords={}, % list of keywords
}


\renewcommand{\lstlistingname}{Code}% Listing -> Code
\renewcommand{\lstlistlistingname}{Liste des \lstlistingname s}% List of Listings -> List of codes



\begin{document}

\thispagestyle{empty}
\maketitle
\tableofcontents
\newpage
\section{Introduction}
Le problème de transport optimal fut étudié dans sa forme primitive par \emph{Gaspard Monge} dans son \emph{Mémoire sur la théorie des déblais et des remblais} de 1781 \cite{monge}. Il s'intéressait au déplacement de tas de matériaux d'un site d'extraction à un site de construction en minimisant le coût de transport (le coût de déplacement d'une unité est le produit de la charge déplacée par la distance). \\
La formulation moderne a été introduite par \emph{Leonid Kantorovitch}, prix Nobel d'économie en 1975 pour sa contribution aux allocations optimales.  \\


Nous essaierons d'être le plus exhaustif possible dans la présentation des résultats, la plupart des énoncés seront démontrés. 


Rendons à Santambrogio ce qui lui appartient, la première section vient en quasi-entièreté de son livre \cite{santambrogio2015optimal}, agrémenté de quelques précisions qui faciliteront la compréhension. 

Le plan de rapport est le suivant, nous commencerons par présenté les notions liées au transport optimal, puis nous ferrons un court aparté sur les opérateurs proximaux. Enfin, Nous attaquerons la discrétisation et la mise en œuvre numérique. 
{\Huge Plan : \\
1 - Théorie sur le transport optimal \\
2 - Théorie sur les opérateurs proximaux  \\
3 - Numérique \\
	3.1 - Idées générales de l'implémentation \\
	3.2 - Implémentation Grosse matrice \\
	3.3 - Implemetation centrée \\
		3.3.1 - 1D \\
		3.3.2 - 2D \\
	3.4 - Implementation staggered FFT \\
4 - Galerie du Fun \\

}
\newpage


\section{Problème de Transport Optimal}
Dans cette section, nous nous attachons à décrire le problème de transport optimal. Nous donnerons des preuves de l'existence de solutions à ce problème. Nous finirons par donner la formulation du problème par Benamou et Brenier, que nous nous attacherons de résoudre numériquement dans la partie \ref{sec:numerique}. 

\subsection{Formulation du problème de Transport Optimal}
Intuitivement, le problème de transport optimal consiste à trouver une méthode pour déplacer un tas de matériaux d'un endroit à un autre, de façon à minimiser un coût, lié au transport de ces matériaux par exemple. C'est l'approche initiale de Monge. 
\begin{figure}[!h]
\centering
\includegraphics[width=\linewidth]{img/tractopelle.png}
\caption{\label{fig:tractopelle}Le transport optimal dans sa version vulgarisée}
\end{figure}\\
Dans des termes plus formels et modernes, les tas de sables représentent des ensembles mesurables dans des espaces métriques, le tractopelle est l'application de transport et le coût est la "distance" entre un ensemble de départ et un ensemble d'arrivée. 
\begin{definition}{Transport}
Soient deux mesures de probabilité $\mu$ sur $(\mathcal{X},\mathcal{B}(\mathcal{X}))$ et $\nu$ sur $(\mathcal{Y},\mathcal{B}(\mathcal{Y}))$ munis de leur tribu borélienne. Un \textbf{transport} est une application $T\ :\ \mathcal{X}\rightarrow\mathcal{Y}$ qui envoie la mesure $\mu$ sur la mesure $\nu$. C'est à dire : 
\begin{align}
\forall B\in\mathcal{B}(\mathcal{Y}),\quad \mu(T^{-1}(B))=\nu(B)
\label{eq:trspmesures}
\end{align}
Cela signifie que $\nu$ est la mesure image de $\mu$ par l'application $T$.\\
Nous noterons $T_{\#}\mu=\nu$, une application $T$ vérifiant \eqref{eq:trspmesures}.
\end{definition}
Pour présager du théorème de Benamou et Brenier, remarquons que la relation \eqref{eq:trspmesures} traduit la conservation de la masse par l'application $T$. La figure \ref{fig:illustrans} illustre cette relation dans le cas où les mesures possèdent des densités. Pour cela, définissons une application permettant de quantifier cette notion de coût de transport. 
\begin{figure}[!h]
\centering
\includegraphics[width=0.7\linewidth]{img/transport.jpg}
\caption{\label{fig:illustrans}Illustration de l'application de transport}
\end{figure}\\
Bien sur, l'application de transport n'est pas unique, il nous faut une manière de sélectionner un transport qui serait "meilleur" que les autres. 
\newpage
\begin{definition}{}
Le coût est une application 
$$
\fonction{C}{\mathcal{X}\times\mathcal{Y}}{[0,+\infty]}{(x,y)}{C(x,y)}
$$
\end{definition}
Cette application représente le coût d'affecter $x$ en $y$. Ces deux définitions nous amènent au problème de transport optimal : \\

\fbox{
  \parbox{\textwidth}{
  Étant données deux mesures de probabilités $\mu$ sur $\mathcal{X}$, $\nu$ sur $\mathcal{Y}$ et une application coût $C$, trouver une application de transport $T$ réalisant le 
	\begin{equation}
	\tag{MP}
	\inf\left\{M(T) := \int_{\mathcal{X}} C(x,T(x))\ d\mu (x), \quad T_{\#}\mu = \nu\right\}
	\label{eq:MP}
	\end{equation}
  }
}
\vspace{0.3cm}

L'existence du transport optimal n'est absolument pas trivial, nous suivrons la présentation du livre de Santambrogio  \cite{santambrogio2015optimal} pour démontrer l'existence d'une application de transport optimal.

\subsection{Plan de Transport Optimal}
Le problème \eqref{eq:MP} est difficile à résoudre du fait de la contrainte. Oublions ce problème pour un temps, et regardons une forme généralisée donnée par Kantorovitch. \\

\fbox{
  \parbox{\textwidth}{
  Étant donné deux mesures de probabilités $\mu$ et $\nu$ et un coût $C$, trouver la mesure $\pi$ réalisant le 
	\begin{equation}
	\tag{KP}
	\inf\left\{ K(\pi ) := \int_{\mathcal{X}\times\mathcal{Y}} C(x,y)\ d\pi (x,y), \quad \pi \in\Pi(\mu,\nu )\right\}
	\label{eq:KP}
	\end{equation}
	Où, $\Pi(\mu,\nu)$ est l'ensemble des plans de transports : 
	\begin{empheq}[left={ \Pi(\mu,\nu) = \empheqlbrace}]{align}
	  &  \pi \text{ une probabilité sur }\mathcal{X}\times\mathcal{Y}\nonumber \\
      &  \forall A\in\mathcal{B}(\mathcal{X}),\quad \pi (A\times\mathcal{Y}) = \mu(A) \label{eq:contrKP}\\
      &  \forall B\in\mathcal{B}(\mathcal{Y}),\quad \pi (\mathcal{X}\times B) = \nu(B)  \nonumber,
	\end{empheq}
  }
}
\vspace{0.3cm}

La valeur de $\pi (A\times B)$ décrit la quantité de matière transportée de $A$ en $B$. Cette description permet des déplacements plus généraux qu'un transport classique. En effet, à partir d'un point $x$, les particules peuvent être transportées en plusieurs destinations. Si c'est le cas, alors cela ne peut pas être décrit au travers d'une application de transport $T$, car localement il n'y a pas conservation de la masse.\\
Remarquons que les contraintes \eqref{eq:contrKP} sont des contraintes de marginales sur la mesure de probabilité $\pi$, cela signifie que nous restreignons notre attention aux plans de transports qui déplacent des particules distribuées selon $\mu$ sur des particules distribuées selon $\nu$.

\begin{figure}[!h]
\centering
\includegraphics[width=0.6\linewidth]{img/transport_plan2.jpg}
\caption{Illustration de la notion de plan de transport\label{fig:plantransp}}
\end{figure}
La figure \ref{fig:plantransp} illustre la notion de plan de transport. Le dégradé de couleur montre que la mesure $\pi$ peut envoyer un point $x\in\mathcal{X}$ sur plusieurs points de $\mathcal{Y}$ dans différentes proportions. \\

Les minimiseurs de ce problèmes sont appelés plans de transport optimal entre $\mu$ et $\nu$. Ce problème est bien une généralisation de \eqref{eq:MP}, puisque si $\pi = (Id,T)$ pour une certaine fonction mesurable $T\ :\ \mathcal{X}\rightarrow\mathcal{Y}$. Alors l'application $T$ serait un transport optimal entre $\mu$ et $\nu$. \\

Nous allons montrer l'existence de solutions pour le problème relaxé \eqref{eq:KP} à l'aide du calcul des variations. Les outils principaux de cette méthode sont rappelés à l'annexe \ref{sec:variations}. 
\begin{theoreme}{}
Soit $\mathcal{X}$ et $\mathcal{Y}$ des espaces métriques compacts. Supposons que le coût $C\ :\ \mathcal{X}\times\mathcal{Y}\rightarrow [0,+\infty]$ soit continue. Alors le problème \eqref{eq:KP} admet une solution.
\end{theoreme}
\begin{preuve}
Il s'agit de montrer que l'ensemble $\Pi (\mu,\nu)$ est compact et que l'application $\pi\rightarrow K(\pi )$ est continue, puis d'appliquer le théorème de Weierstrass \eqref{thm:weierstrass}. \\
Nous choisissons la notion de convergence donnée par la convergence faible des mesures de probabilités (voir \eqref{thm:prokhorov} pour la dualité avec $C_b(\mathcal{X}\times\mathcal{Y})$ ce qui ici est a même chose que $C(\mathcal{X}\times\mathcal{Y})$ ou $C_0(\mathcal{X}\times\mathcal{Y})$ car $\mathcal{X}$ et $\mathcal{Y}$ sont compacts). Cela nous donne la continuité de $K$ puisque $C\in C(\mathcal{X}\times\mathcal{Y})$. \\

Soit une suite $\pi_n\in\Pi (\mu,\nu )$. Ce sont des mesures de probabilités, donc de masse $1$ et sont donc bornée dans le dual de $C(\mathcal{X}\times\mathcal{Y})$. Ainsi, la compacité faible - * dans les espaces duaux, garanti l'existence d'une sous suite $\pi_{n_k}$ convergeant faiblement vers une probabilité $\pi$. 
Vérifions que $\pi \in \Pi (\mu,\nu )$. \\
Fixons $\phi\in C(\mathcal{X})$, comme $\int\phi\ d\pi_{n_k} =\int \phi\ d\mu $,par passage à la limite, nous avons : $\int\phi\ d\pi =\int\phi\ d\mu$. De même nous montrons que la seconde contrainte de marginale \eqref{eq:contrKP} est vérifiée. \\
Ainsi, l'espace $\Pi(\mu,\nu)$ est compact et le théorème de Weierstrass s'applique.
\end{preuve}

Allons vers plus de généralités. 

\begin{proposition}
Supposons que le coût soit semi continue inférieurement et bornée inférieurement. Alors le problème \eqref{eq:KP} admet une solution. 
\end{proposition}
\begin{preuve}
La seule différence avec le cas précédent est que $K$ n'est plus continue mais semi continue inférieurement pour la convergence faible des mesures de probabilités. Cela vient du lemme suivant appliqué à $f = C$ et $\Omega = \mathcal{X}\times\mathcal{Y}$.
\begin{lemme}
Si $f\ :\ \Omega\rightarrow \RR\cup\{+\infty\}$ est une fonction semi-continue inférieurement, bornée inférieurement sur un espace métrique $\Omega$. Alors la fonctionnel $J \ :\ \mathcal{M}_+(\Omega )\rightarrow \RR\cup\{+\infty\}$ définie sur les mesures de Radon positives par $J(\lambda) =\int f d\lambda$ est semi continue inférieurement pour la convergence faible des mesures. 
\end{lemme}
\begin{preuve}
Soit une suite $(f_k)$ de fonctions continues, bornées convergeant vers $f$ par valeurs croissantes. 
Soit $J_k(\lambda ) =\int f_k\ d\lambda$, $J_k\rightarrow J$ par convergence monotone. Nous pouvons alors écrire $J(\lambda )=\sup_k J_k(\lambda )$.
Alors $J$ est semi-continue inférieurement comme le $\sup$ de fonctionnelles continues.
\end{preuve}
Et donc, le théorème de Weierstrass \eqref{thm:weierstrass} s'applique.
\end{preuve}

\begin{proposition}\label{prop:existKP3}
Supposons que $\mathcal{X}$ et $\mathcal{Y}$ soient des espaces métriques complets et séparables (i.e. polonais) et que le coût $C$ est semi-continue inférieurement et bornée inférieurement. Alors le problème \eqref{eq:KP} admet une solution. 
\end{proposition}
\begin{preuve}
Cette fois, c'est la compacité de $\Pi(\mu,\nu)$ qui est moins évidente. Nous allons utiliser le théorème de Prokhorov \eqref{thm:prokhorov}. Ce qui revient à montrer que toute suite de $\Pi (\mu,\nu)$ est tendue. \\
Soient $\epsilon >0$ et deux compact $K_{\mathcal{X}}\subset\mathcal{X}$ et $K_{\mathcal{Y}}\subset \mathcal{Y}$ tels que $\mu (\mathcal{X}\backslash K_{\mathcal{X}})$, $\nu (\mathcal{Y} \backslash K_{\mathcal{Y}}) < \frac{1}{2}\epsilon$. Cela est possible par la réciproque du théorème de Prokhorov, car une mesure seule est toujours tendue. 
Alors l'ensemble $K_{\mathcal{X}}\times K_{\mathcal{Y}}$ est compact dans $\mathcal{X}\times\mathcal{Y}$, et pour toute suite $\pi_n\in\Pi (\mu,\nu )$, nous avons 
\begin{align*}
\pi_n\left((\mathcal{X}\times\mathcal{Y})\backslash (K_{\mathcal{X}}\times K_{\mathcal{Y}}) \right) &\leq \pi_n((\mathcal{X}\backslash K_{\mathcal{X}})\times \mathcal{Y}) + \pi_n(\mathcal{X}\times (\mathcal{Y}\backslash K_{\mathcal{Y}}))\\
&\leq \mu (\mathcal{X} \backslash K_{\mathcal{X}}) + \nu (\mathcal{Y}\backslash K_{\mathcal{Y}}) < \epsilon
\end{align*}
Ce qui montre la tension et donc la compacité de toute suite de $\Pi (\mu,\nu )$. 
\end{preuve}

\subsection{Problème dual au plan de Transport Optimal}
Maintenant que nous avons démontré l'existence de solution au problème de Kantorovitch, nous souhaiterions savoir sous quelles conditions nous pouvons remonter au problème de Monge. Comme le problème \eqref{eq:KP} est un problème d'optimisation linéaire sous contraintes d'égalité, il est donc naturel d'étudier le problème dual. \\

Toujours suivant la résentation du livre de Santambrogio \cite{santambrogio2015optimal}, cherchons en premier lieu le problème dual formellement. \\
La contrainte $\pi\in\Pi(\mu,\nu)$ peut s'exprimer sous la forme : 
\begin{align}
\sup_{(\phi,\psi)\in C_b(\mathcal{X})\times C_b(\mathcal{Y})}& \int_{\mathcal{X}} \phi(x)\ d\mu (x) + \int_{\mathcal{Y}}\psi(y)\  d\nu(y) - \int_{\mathcal{X}\times\mathcal{Y}}\phi(x)+\psi(y)\ d\pi(x,y)\label{eq:sup1}\\
&= \left\{\begin{array}{cl}
0 & \text{si } \pi \in\Pi(\mu,\nu)\\
+\infty & \text{sinon}
\end{array}\right.\nonumber
\end{align}
Ainsi nous pouvons supprimer la contrainte dans le problème \eqref{eq:KP} en ajoutant l'expression \eqref{eq:sup1}, puisque si la contrainte est satisfaite, rien n'est ajoutée, si elle ne l'est pas nous obtenons $+\infty$ ce qui sera évité par la minimisation. 
$$
\inf_{\pi} \int_{\mathcal{X}\times\mathcal{Y}} C\ d\pi + \sup_{(\phi,\psi)\in C_b(\mathcal{X})\times C_b(\mathcal{Y})} \int_{\mathcal{X}} \phi\ d\mu + \int_{\mathcal{Y}}\psi\  d\nu - \int_{\mathcal{X}\times\mathcal{Y}}\phi(x)+\psi(y)\ d\pi(x,y)
$$
Il n'est pas toujours possible d'échanger le $\sup$ et l'$\inf$, mais supposons pour le moment que cela soit possible : 
\begin{align}
\sup_{(\phi,\psi)\in C_b(\mathcal{X})\times C_b(\mathcal{Y})} \int_{\mathcal{X}} \phi\ d\mu + \int_{\mathcal{Y}}\psi\  d\nu + \inf_{\pi} \int_{\mathcal{X}\times\mathcal{Y}}(C(x,y) - \phi(x)+\psi(y))\ d\pi(x,y)
\label{eq:infsupexchange}
\end{align}

Avec ce problème de maximisation sur les variables $(\phi,\psi)$, l'infimum en $\pi$ peut être réécrit comme une contraintes sur $(\phi,\psi)$ : 
\begin{align}
\inf_{\pi} \int_{\mathcal{X}\times\mathcal{Y}}(C - \phi\oplus\psi) \ d\pi = 
\left\{
\begin{array}{cl}
0 & \text{si } \phi\oplus\psi\leq C \text{ sur } \mathcal{X}\times\mathcal{Y}\\
-\infty &\text{ sinon}
\end{array}
\right.
\end{align}
Où la fonction, $\phi\oplus\psi$ est définie par $(\phi\oplus\psi)(x,y) = \phi(x)+\psi(y)$. L'égalité précédente est justifiée car, si $\phi\oplus\psi>C$ quelque part alors en utilisant des mesures $\pi$ concentrée en cet endroit et de masse croissante, l'intégrale tend vers $-\infty$. Ce qui nous amène au problème suivant : \\

\fbox{
  \parbox{\textwidth}{
  Étant donné deux mesures de probabilités $\mu$ et $\nu$ et un coût $C$, trouver deux fonctions continues, bornée $\phi$ et $\psi$ réalisant le 
	\begin{equation}
	\tag{DKP}
	\sup\left\{\int_{\mathcal{X}} \phi\ d\mu + \int_{\mathcal{Y}}\psi\ d\nu, \quad  (\phi,\psi)\in C_b(\mathcal{X})\times C_b(\mathcal{Y}),\quad \phi\oplus\psi \leq C \right\}
	\label{eq:DKP}
	\end{equation}
  }
}
\vspace{0.3cm}

Remarquons que le $\sup\eqref{eq:DKP} \leq \min\eqref{eq:KP}$, en intégrant la condition $\phi\oplus\psi \leq C$ par rapport à $\pi$ : 
$$
\int_{\mathcal{X}} \phi\ d\mu  + \int_{\mathcal{Y}}\psi\ d\nu = \int_{\mathcal{X}\times\mathcal{Y}} \phi\oplus\psi\ d\pi \leq \int_{\mathcal{X}\times\mathcal{Y}} C\ d\pi
$$

Malheureusement le problème \eqref{eq:DKP} n'admet pas trivialement l'existence de ses maximums, car l'espace des fonctions continues bornées n'est pas compact. Nous montrerons dans un premier temps l'existence de ces maximums, puis montrerons que le problème \eqref{eq:DKP} est bien le dual de \eqref{eq:DKP} au sens ou $\max\eqref{eq:DKP} =\min\eqref{eq:KP}$.


\begin{definition}{C et $\bar{C}$-transformée, C et $\bar{C}$-concavité}
Soit une fonction $\chi\ :\ \mathcal{X}\rightarrow [-\infty,+\infty]$. La C-transformée (ou parfois, C - conjuguée) de $\chi$ est définie par $\chi^C\ : \ \mathcal{Y}\rightarrow[-\infty,+\infty]$ : 
$$
\chi^C(y)=\inf_{x\in\mathcal{X}} C(x,y) -\chi(x)
$$
De même, la $\bar{C}$-transformée est définie pour une fonction $\tau\ :\ \mathcal{Y}\rightarrow [-\infty,+\infty]$ par 
$$
\tau^{\bar{C}}(x) = \inf_{y\in\mathcal{Y}} C(x,y) -\tau(y)
$$
De plus, nous dirons qu'une fonction $\psi$ définie sur $\mathcal{Y}$ est $\bar{C}$-concave s'il existe $\chi\ :\ \mathcal{X}\rightarrow [-\infty,+\infty]$ telle que $\psi = \chi^C$. Respectivement, une fonction $\phi$ définie sur $\mathcal{X}$ est dite $C$-concave si il existe $\tau\ :\ \mathcal{Y}\rightarrow [-\infty,+\infty]$ telle que $\phi=\tau^{\bar{C}}$. \\
Nous noterons $C$-conc($\mathcal{X}$) et $\bar{C}$-conc($\mathcal{Y}$) les ensembles des fonctions $C$ et $\bar{C}$-concaves. 
\end{definition}

Dans notre cas, $C$ est continue sur un ensemble compact, et est donc uniformément continue. C'est à dire qu'il existe une fonction continue croissante $\omega\ :\ \RR^+\rightarrow\RR^+$ et $\omega(0)=0$ (module de continuité) telle que 
$$
\left|C(x,y) - C(x',y')\right| \leq \omega(d(x,x') + d(y,y'))
$$
En reprenant la définition de $\chi^C(y) = \inf_x g_x(y)$ avec $g_x(y) = C(x,y)-\chi(x)$ qui vérifie $|g_x(y)-g_x(y')|\leq\omega(d(y,y'))$. Ce qui montre que $\chi^C$ et $C$ partagent le même module de continuité. \\

Ensuite remarquons qu'étant donnée une paire $(\phi,\psi)$ du problème \eqref{eq:DKP}, elle peut toujours être remplacée par la paire $(\phi,\phi^C)$ puis par $(\phi^{C\bar{C}},\phi^C)$, car les contraintes sont préservées et les intégrales augmentées. 
Nous pourrions espérer itérer ce processus, mais pour toute fonction $\phi$, nous avons $\phi^{C\bar{C}C} = \phi$. Cependant les considérations précédentes nous amènent le théorèmes d'existence suivant. 
\begin{theoreme}{}
\label{thm:existDKP}
Supposons que $\mathcal{X}$ et $\mathcal{Y}$ soient compact et que $C$ est continue. Alors il existe une solution $(\phi,\psi)$ au problème \eqref{eq:DKP}, de la forme $\phi\in C$-conc($\mathcal{X}$), $\psi\in \bar{C}$-conc($\mathcal{Y}$) et $\psi=\phi^C$. En particulier, 
\begin{align}
\max\eqref{eq:DKP} = \max_{\phi\in C\text{-conc}(\mathcal{X})}\int_{\mathcal{X}}\phi\ d\mu +\int_{\mathcal{Y}} \phi^C\ d\nu
\label{eq:existenceDKP}
\end{align}
\end{theoreme}
\begin{preuve}
Prenons une suite maximisante $(\phi_n,\psi_n)$ et améliorons là par la $C$ et la $\bar{C}$-transformée. Par les considérations précédentes, nous pouvons supposer une borne uniforme sur la continuité de ces fonctions (le même module de continuité que $C$). Gardons la notation $(\phi_n,\psi_n)$  pour désigner cette suite améliorée. \\
Vérifions que cette suite est équibornée pour appliquer le théorème d'Ascoli Arzélà \eqref{thm:ascoli}. Remarquons qu'ajouter une constante à $\phi$ et la retirer à $\psi$ ne change pas la valeur de la fonctionnelle et n'affecte pas les contraintes. Ainsi, comme $\phi_n$ est continue sur un ensemble compact, elle est donc bornée, quitte à ajouter le minimum, nous pouvons supposer que $\min \phi_n=0$, nous avons aussi que $\max \phi_n \leq \omega (d(\mathcal{X}))$, car les oscillations d'une fonctions sont toujours plus faible que son module de continuité appliqué à la plus grande distance possible dans l'ensemble.\\
Comme nous avons choisis $\psi_n=\phi_n^C$, nous avons également, $\psi_n(y)=\inf_xC(x,y)-\phi_n(x)\in[\min C- \omega (d(\mathcal{X})),\max C]$. Ce qui nous donne des bornes uniformes sur $(\phi_n,\psi_n)$ et nous permet d'appliquer le théorème d'Ascoli - Arzelà.\\
A une sous suite près, $\phi_n\rightarrow\phi$ et $\psi_n\rightarrow\psi$ avec convergence uniforme, de ce fait : 
\begin{align}
\int_{\mathcal{X}}\phi_n\ d\mu + \int_{\mathcal{Y}}\psi_n\ d\nu \rightarrow \int_{\mathcal{X}}\phi\ d\mu + \int_{\mathcal{Y}}\psi\ d\nu
\end{align}
De plus, 
$$
\phi_n(x)+\psi_n(y)\leq C(x,y) \Rightarrow \phi(x)+\psi(y)\leq C(x,y)
$$
Ici la convergence simple aurait suffit. Cela montre que $(\phi,\psi)$ est une paire admissible pour \eqref{eq:DKP} et donc optimale.
\end{preuve}
Avant de montrer que les problèmes \eqref{eq:DKP} et \eqref{eq:KP} sont bien duaux, introduisons un outils, la transformée de Legendre Flenchel et donnons quelques propriétés. 
\begin{definition}{Transformée de Legendre Flenchel}
Pour toute fonction $f\ :\ E\rightarrow \RR\cup\{+\infty\}$ sa transformée de Legendre Flenchel est définie par :
\begin{align}
\fonction{f^{\star}}{E^{\star}}{ \RR\cup\{+\infty\}}{y}{\sup_{x\in E}\langle x,y\rangle -f(x)}
\end{align}
\end{definition}
\begin{propriete}
\begin{enumerate}
\item Une fonction $f$ est convexe et semi continue inférieurement si et seulement si il existe une fonction $g$ telle que $f=g^{\star}$.
\item Une fonction $f\ :\ E\rightarrow \RR\cup\{+\infty\}$ est convexe et semi continue inférieurement si et seulement si $f^{\star\star}=f$.
\end{enumerate}
\end{propriete}
Montrons à présent que \eqref{eq:DKP} est bien le problème dual associé à \eqref{eq:KP}. 
\begin{theoreme}{}
\label{thm:dualite}
Les problèmes \eqref{eq:KP} et \eqref{eq:DKP} sont bien duaux l'un de l'autre :
\begin{align}
\min\eqref{eq:KP} = \max\eqref{eq:DKP}
\end{align}
\end{theoreme}
\begin{preuve}
Pour tout $p\in C(\mathcal{X}\times\mathcal{Y})$, posons 
$$
H(p) = -\max\left\{\int_{\mathcal{X}} \phi\ d\mu +\int_{\mathcal{Y}} \psi\ d\nu, \quad \phi\oplus\psi\leq C-p\right\}
$$
C'est l'opposée de la valeur de \eqref{eq:DKP} pour le coût $C-p$. Par le théorème \ref{thm:existDKP}, nous avons que, $H(p)$ est bien atteint et que le module de continuité de la paire optimale $(\phi,\psi)$ est le même que celui de $C-p$. 
\begin{lemme}
La fonction $H\ :\ C(\mathcal{X}\times\mathcal{Y})\rightarrow\RR$ est convexe et semi continue inférieurement pour la convergence uniforme.
\end{lemme}
\begin{preuve}
Pour la convexité, prenons $p_0$ et $p_1$ associé avec leur paire optimale $(\phi_0,\psi_0)$ et $(\phi_1,\psi_1)$. Pour $t\in [0,1]$ posons, $p_t=(1-t)p_t+tp_1$, $\phi_t=(1-t)\phi_0 + t\phi_1$ et $\psi_t=(1-t)\psi_0+t\psi_1$. La paire $(\phi_t,\psi_t)$ est admissible dans le maximum définissant $H(p_t)$. Ainsi, 
$$
H(p_t)\leq-\left(\int_{\mathcal{X}}\phi_t\ d\mu+\int_{\mathcal{Y}}\psi_t d\nu\right) = (1-t)H(p_0)+tH(p_1)
$$
Ce qui montre la convexité.\\
Pour la semi continuité, prenons une suite $(p_n)$ convergeant uniformément vers $p$. A une sous suite près, $p_n$ réalise la $\liminf$ de $H(p_n)$. De la convergence uniforme, la suite $p_n$ est équicontinue et bornée (réciproque de théorème d'Ascoli - Arzelà \eqref{thm:ascoli}). Ainsi, les paires optimales correspondantes $(\phi_n,\psi_n)$ sont aussi équicontinues et bornées. A une sous suite près, supposons que $\phi_n\rightarrow\phi$ et $\psi_n\rightarrow\psi$ uniformément. Comme $\phi_n\oplus\psi_n\leq C-p$, nous avons $\phi\oplus\psi\leq C-p$, ainsi :
$$
H(p) \leq - \left(\int_{\mathcal{X}}\phi\ d\mu+\int_{\mathcal{Y}}\psi\ d\nu\right) = \lim_n H(p_n)=\liminf_n H(p_n)
$$
ce qui montre la semi continuité inférieure.
\end{preuve}
Calculons à présent la transformée de Legendre Flenchel de H.\\
$H^{\star}\ :\ \mathcal{M}(\mathcal{X}\times\mathcal{Y})\rightarrow\RR\cup\{+\infty\}$. Pour tout $\pi\in\mathcal{M}(\mathcal{X}\times\mathcal{Y})$, nous avons 
$$
H^{\star}(\pi)= \sup_p \int_{\mathcal{X}\times\mathcal{Y}} p\ d\pi + \sup \left\{ \int_{\mathcal{X}}\phi\ d\mu + \int_{\mathcal{Y}}\psi\ d\nu, \quad \phi\oplus\psi\leq C- p \right\}
$$
Ce qui se réécrit sous la forme d'un $\sup$ unique sur $p,\ \phi,\ \psi$. Remarquons que, si $\pi\notin\mathcal{M}_+(\mathcal{X}\times\mathcal{Y})$, alors il existe $p_0\leq 0$ tel que $\int p_0\ d\pi >0$, en prenant $\phi\equiv 0$, $\psi\equiv 0$ et $p=C+np_0$ en faisant tendre $n$ vers $+\infty$ nous obtenons $H^{\star}(\pi)=+\infty$.\\
D'autre part, si $\pi\in\mathcal{M}_+(\mathcal{X}\times\mathcal{Y})$, en prenant le $p$ le plus grand possible i.e. $p=C-\phi-\psi$, nous obtenons 
$$
H^{\star}(\pi)=  \sup_{(\phi,\psi)} \int_{\mathcal{X}\times\mathcal{Y}} C\ d\pi + \int_{\mathcal{X}}\phi\ d\mu + \int_{\mathcal{Y}}\psi\ d\nu - \int_{\mathcal{X}\times\mathcal{Y}} \phi\ d\pi -\int_{\mathcal{X}\times\mathcal{Y}} \psi\ d\pi
$$
C'est exactement l'expression obtenue au \eqref{eq:sup1} pour réécrire les contraintes de \eqref{eq:KP}. 
$$
H^{\star}(\pi)=\left\{
\begin{array}{cl}
K(\pi) & \text{si } \pi \in\Pi(\mu,\nu)\\
+\infty & \text{sinon}
\end{array}
\right.
$$

La preuve est presque terminée, $\max\eqref{eq:DKP} = -H(0) =- H^{\star\star}(0)$ car $H$ est convexe et semi continue inférieurement. De plus, $H^{\star\star}(0)=\sup_{\pi}\langle 0,\pi\rangle-H^{\star}(\pi)=-\inf H^{\star}=-\min\eqref{eq:KP}$.
\end{preuve}

Ainsi, nous voyons que 
$$
\min\eqref{eq:KP}= \max_{\phi\in C\text{-conc}(\mathcal{X})}\int_{\mathcal{X}}\phi\ d\mu +\int_{\mathcal{Y}} \phi^C\ d\nu
$$
Ce qui montre que la valeur du minimum de \eqref{eq:KP} est une fonction convexe de $\mu$ et $\nu$, comme supremum de fonctionnelle linéaire. 
\begin{definition}{Potentiel de Kantorovitch}
Les fonctions $\phi$ réalisant le maximum de \eqref{eq:existenceDKP} sont appelés potentiels de Kantorovitch pour le transport de $\mu$ sur $\nu$.
\end{definition}

\subsection{Existence du transport optimal}

A partir des résultats précédents sur le problèmes \eqref{eq:KP} et son dual \eqref{eq:DKP}, nous allons montrer l'existence du transport optimal dans le cas où, $\mathcal{X}=\mathcal{Y}=\Omega\subset\RR^n$ compact et $C(x,y)=h(x-y)$ pour une certaine fonction strictement convexe $h$. 

Rappelons que pour une mesure $\pi$, son support est le plus petit ensemble fermé sur lequel $\pi$ est concentré : 
$$
\supp \pi = \bigcap \left\{A\ |\ A \text{ fermé },\ \pi(\mathcal{X}\backslash A) = 0 \right\}
$$

De l'égalité entre le minimum de \eqref{eq:KP} et le maximum de \eqref{eq:DKP} et le fait que le deux soient atteints, considérons un plan de transport optimal $\pi$ et un potentiel de Kantorovitch $\phi$, 
\begin{align}
\phi(x)+\phi^C(y) &\leq C(x,y) \text{ sur } \Omega\times\Omega \\
\phi(x)+\phi^C(y) &= C(x,y) \text{ sur }\supp\pi
\end{align}
L'égalité vient de l'inégalité (qui est vraie partout) et de 
\begin{align}
\min\eqref{eq:KP} &= \max \eqref{eq:DKP}\nonumber\\
\int_{\Omega\times\Omega} C\ d\pi &= \int_{\Omega}\phi\ d\mu +  \int_{\Omega} \phi^C\ d\nu = \int_{\Omega\times\Omega} \phi\oplus\phi^c\ d\pi
\end{align}
ce qui implique l'égalité $\pi$ presque partout. 

\begin{proposition}
Si $C\in C^1(\Omega\times\Omega,[0,+\infty])$, $\phi$ un potentiel de Kantorovitch pour le coût $C$ pour le transport de $\mu$ vers $\nu$. Soit $(x_0,y_0)\in\supp\pi$. Alors, si $\phi$ est différentiable en $x_0$, nous avons $\nabla\phi(x_0)= \frac{\partial C}{\partial x}(x_0,y_0)$.\\
En particulier les gradients de deux potentiels de Kantorovitch coïncident en tout point $x_0\in\supp\pi$ où ils sont tout les deux différentiables.
\end{proposition}
\begin{preuve}
Soit $(x_0,y_0)\in\supp\pi$ telle que $\phi$ soit différentiable en $x_0$, des calculs précédents, nous déduisons que l'application $x\mapsto\phi(x)-C(x,y_0)$ atteint sont minimum en $x=x_0$. 
Comme $\phi$ et $C(\cdot,y_0)$ sont différentiables, nous obtenons directement le résultat souhaité. 
\end{preuve}


\begin{theoreme}{Existence du transport optimal}
\label{thm:existtrasp}
Soient $\mu$ et $\nu$ des mesures de probabilités sur une domaine compact $\Omega$ de $\RR^n$, alors il existe un plan de transport optimal $\pi$ pour le coût $C(x,t) = h(x-y)$, avec $h$ strictement convexe. \\
De plus, $\pi$ est unique, de la forme $(Id,T)_{\#}\mu$ si $\mu$ est absolument continue et $\partial\Omega$ est $\mu$ - négligeable. Enfin, il existe un potentiel de Kantorovitch $\phi$ qui est lié à $T$ par :
\begin{align}
T(x) = x- (\partial h)^{-1}(\nabla\phi(x))
\end{align}
\end{theoreme}

\begin{preuve}
Le théorème \ref{thm:dualite} nous donne l'existence d'un plan de transport optimal $\pi$ et d'un potentiel de Kantorovitch $\phi$.\\

$\phi$ partage le même module de continuité que $C$, c'est une fonction lipschitzienne sur $\Omega\times\Omega$ puisque $h$ est localement lipschitzienne et bornée sur $\Omega$. Ainsi, $\phi$ est aussi lipschitzienne.\\

Pour $C(x,y) = h(x-y)$,si $h$ est différentiable, nous obtenons par la proposition précédente que $\nabla\phi (x_0)=\nabla h(x_0-y_0)$, si ce n'est pas le cas, nous avons $\nabla\phi(x_0)\in\partial h(x_0-y0)$ le sous différentiel de $h$\footnote{Nous renvoyons à la section \ref{sec:prox} pour plus de détails et de résultats sur les sous-différentiels}. Comme $h$h est strictement convexe, $(\partial h)^{-1}$ existe, et nous avons 
$$
x_0-y_0=(\partial h)^{-1}(\nabla\phi(x_0))
$$
Comme $\mu$ est absolument continue, $\partial\Omega$ et l'ensemble des points ou $\phi$ n'est pas différentiable sont négligeables (par le théorème de Rademacher). \\

Cela montre que le plan de transport optimal est induit par un transport, que ce transport est donnée par $x\mapsto x -(\partial h)^{-1}(\nabla\phi(x_0))$. Ainsi il est uniquement déterminé, car $\phi$ ne dépend pas de $\pi$. En conséquence, $\pi$ est également unique. 
\end{preuve}
Ainsi, tout les coûts de la forme $C(x,y) =\|x-y\|^p$ avec $p>1$ sont traités. 


\subsection{Le cas quadratique sur $\RR^n$}
Comme souvent le cas quadratique possède quelques propriétés supplémentaires qu'il convient d'exposer. Mettons nous dans $\RR^n $ avec le coût $C(x,y)=\frac{1}{1}\|x-y\|^2$.\\
Premièrement, si nous sommes sur un ensemble compact, nous pouvons préciser le théorème \eqref{thm:existtrasp}. Il existe une application de transport optimal qui s'exprime comme 
$$
T(x) = x -\nabla\phi(x) = \nabla\left(\frac{x^2}{2}-\phi(x)\right) = \nabla u(x)
$$
Pour une certaine fonction convexe $\phi$. Adaptons le résultat précédent aux domaines non bornés. Considérons pour cela une variante du problème \eqref{eq:DKP} : \\

\fbox{
  \parbox{\textwidth}{
  Étant donné deux mesures de probabilités $\mu$ et $\nu$ et un coût $C$, trouver deux fonctions $\phi$ et $\psi$ réalisant le 
	\begin{equation}
	\tag{DKP - var}
	\sup\left\{\int_{\RR^n} \phi\ d\mu + \int_{\RR^n}\psi\ d\nu, \quad  (\phi,\psi)\in L^1(\RR^n,\mu)\times L^1(\RR^n,\nu),\quad \phi\oplus\psi \leq C \right\}
	\label{eq:DKPvar}
	\end{equation}
  }
}
\vspace{0.3cm}
Introduisons un lemme technique établissant un lien entre la $C$ - concavité et la transformée de Legendre Flenchel dans le cas du coût quadratique. 

\begin{lemme}
Soit une fonction $\chi\ :\ \RR^n \rightarrow \RR\cup\{+\infty\}$. Posons, $\fonction{u_{\chi}}{\RR^n}{\RR\cup\{+\infty\}}{x}{\frac{1}{2}\|x\|^2-\chi(x)} $. Alors, $u_{\chi^C} = (u_{\chi})^{\star}$. En particulier, une fonction $\chi$ est $C$ - concave si et seulement si $x\mapsto\frac{1}{2}\|x\|^2-\chi(x)$ est convexe et semi continue inférieurement. 
\end{lemme}
\begin{preuve}
Pour le premier point, calculons 
$$
u_{\chi^C}(x) =\frac{1}{2}\|x\|^2 - \chi^C(x)=\sup_y \frac{1}{2}\|x\|^2  - \frac{1}{2}\|x - y\|^2 + \chi(y) = \sup_y \langle x,y\rangle - \left( \frac{1}{2}\|y\|^2 - \chi(y)\right) 
$$
De plus, comme les fonctions $C$ - concaves sont caractérisés par le fait qu'elles sont des $C$ - transformée et des fonctions convexes semi continues inférieurement (comme le $\sup$ de fonctions affines). Ce qui montre le second point.
\end{preuve}
Nous pouvons à présent énoncer le résultat de dualité. 
\begin{theoreme}{}
Soient $\mu$ et $\nu$ des probabilités sur $\RR^n$ et $C(x,y)=\frac{1}{2}\|x-y\|^2$. Supposons que : 
\begin{enumerate}
\item $\int\|x\|^2dx$, $\int\|y\|^2dy<+\infty$,
\item $\mu$ ne donne pas de masse aux hypersurfaces de classe $C^2$.
\end{enumerate}
Alors \eqref{eq:DKPvar} admet une solution $(\phi,\psi)$ et les fonctions $x\mapsto\frac{1}{2}\|x\|^2-\phi(x)$ et $y\mapsto\frac{1}{2}\|y\|^2-\psi(y)$ sont conjuguées l'une de l'autre pour la transformation de Legendre Flenchel.\\
De plus $\sup\eqref{eq:DKPvar}=\inf\eqref{eq:KP}$. 
\end{theoreme}
Comme nous l'avions fait pour \eqref{eq:DKP}, remarquons que $\sup\eqref{eq:DKPvar}\leq \min\eqref{eq:KP}$, en intégrant la condition $\phi\oplus\psi\leq C$ par rapport à $\pi$ : 
$$
\int_{\RR^n}\phi\ d\mu + \int_{\RR^n}\psi\ d\nu = \int_{\RR^n\times\RR^n} \phi\oplus\psi\ d\pi \leq\int_{\RR^n\times\RR^n} C d\pi 
$$


\begin{preuve}
La continuité de $C$ et les propriété de complétude et de séparabilité de $\RR^n$ nous garantissent l'existence du $\min\eqref{eq:KP}$ par la proposition \eqref{prop:existKP3}. Soit $\pi$ la mesure réalisant ce minimum. \\

Il existe alors une paire $(\phi,\psi)$ avec $\phi$ est $C$ - concave et telle que $\phi(x)+\phi^C(y) = C(x,y)$ pour $(x,y)$ dans le support du plan de transport optimal.\\

Alors, nous déduisons que $x\mapsto\frac{1}{2}\|x\|^2-\phi(x)$ et $y\mapsto\frac{1}{2}\|y\|^2-\psi(y)$ sont convexes, conjuguées l'une de l'autre. En particulier, $\frac{1}{2}\|x\|^2-\phi(x)$ est bornée inférieurement par une fonction linéaire, et don $\phi$ est bornée supérieurement par un polynôme du second degré. Cela montre que $\phi_+\in L^1(\mu)$ par hypothèse sur $\mu$, de même, nous montrons que $\psi_+\in L^1(\nu)$. \\
Intégrons à présent $\phi\oplus\psi$ par rapport à $\pi$. 
$$
\int_{\RR^n}\phi\ d\mu + \int_{\RR^n}\psi\ d\nu = \int_{\RR^n\times\RR^n}\phi\oplus\psi\ d\pi = \int_{\RR^n\times\RR^n}C\ d\pi \geq 0
$$
Ce qui prouve que $\int_{\RR^n}\phi\ d\mu,\ \int_{\RR^n}\psi\ d\nu > -\infty$ et donc $\phi\in L^1(\mu)$ et $\psi \in L^1(\nu)$.\\

Ainsi, nous pouvons utilise $(\phi,\phi^C)$ comme paire admissible pour \eqref{eq:DKPvar}. 
$$
\eqref{eq:DKPvar} \geq \int_{\RR^n}\phi\ d\mu + \int_{\RR^n}\phi^C\ d\nu = \int_{\RR^n\times\RR^n}C\ d\pi = \eqref{eq:KP}
$$
par l'optimalité de $\pi$. \\
Ainsi, $\max\eqref{eq:DKPvar} = \min\eqref{eq:KP}$. 
\end{preuve}


\begin{theoreme}{}
Soient $\mu$ et $\nu$ des probabilités sur $\RR^n$ et $C(x,y)=\frac{1}{2}\|x-y\|^2$. Supposons que : 
\begin{enumerate}
\item $\int\|x\|^2dx$, $\int\|y\|^2dy<+\infty$
\item $\mu$ ne donne pas de masse aux hypersurfaces de classe $C^2$.
\end{enumerate}
alors il existe une unique application de transport optimal $T$, donnée par $T=\nabla u$ pour une fonction convexe $u$.
\end{theoreme}

\begin{preuve}
Il existe un plan de transport optimal $\pi$, et par le théorème précédent, une paire $(\phi,\psi)\in L^1(\mu)\times L^1(\nu)$ optimale. \\
Comme $\phi\in L^1(\mu)$, nous en déduisons que $u$ est finie $\mu$ presque partout et comme $u$ est convexe, $\supp\pi\subset \{u<+\infty\}$, un ensemble convexe.\\
Remarquons que $\partial \{u<+\infty\}$ s'exprime comme le graphe d'une fonction concave, c'est donc une hypersurface de classe $C^2$, qui est $\mu$ négligeable par hypothèse. Dans l'intérieur de $\partial \{u<+\infty\}$, $u$ est différentiable $\mu$ presque partout par hypothèse sur $\mu$ et $u$ convexes. \\
Comme $\phi$ est différentiable $\mu$ presque partout, la mesure $\pi$ est concentrée sur le graphe de $x\mapsto x-\nabla\phi(x)=\nabla u(x)$.
\end{preuve}



 

\newpage


Maintenant que nous avons montré l'existence du transport optimal, introduisons petit à petit des hypothèses simplificatrices mais pas aberrantes dans le contexte de la résolution numérique. Mettons nous dans $\RR^n$ et supposons que les mesures aient une densité : $\mu = f_0\ dx$ et $\nu = f_1\ dx$. La relation \eqref{eq:trspmesures} devient alors : 
\begin{align*}
\forall B \in\mathcal{B}(\RR^n),\quad \int_B f_1(y)dy&=\int_{T^{-1}(B)} f_0(x) dx \\
&= \int_B \sum_{x\in T^{-1}(y)} \left(\frac{f_0(x)}{|det\ \nabla T(x)|}\right)dy\\
\end{align*}
C'est à dire $f_1(y) = \sum_{x\in T^{-1}(y)} \left(\frac{f_0(x)}{|det\ \nabla T(x)|}\right)$.\\
Supposons de plus que le transport est injectif et lisse, i.e. $x\in T^{-1}(y) \Leftrightarrow y=T(x)$. Nous obtenons alors l'équation dite de Jacobienne :
 
\begin{equation}
\tag{J}
f_1(T(x)) = \frac{f_0(x)}{|det\ \nabla T(x)|}
\label{eq:jacobienne}
\end{equation}


Notons $\mathcal{T}(f_0,f_1)$ l'ensemble des applications qui vérifient \eqref{eq:jacobienne}.
\begin{definition}{Métrique de Wasserstein}
Pour les coûts de transport de la forme $C(x,y) = \|x - y\|^p$, nous pouvons définir une métrique entre $f_0$ et $f_1$ par
\begin{align*}
W(f_0,f_1)^p=\min_{T\in\mathcal{T}(f_0,f_1)} \int \|x-T(x)\|f_0(x)\ dx
\end{align*} 

\end{definition}


Dans le cas quadratique, $T$ s'écrit comme le gradient d'une fonction convexe, qui satisfait alors l'équation dite de Monge Ampère : 
\begin{equation}
\tag{MA}
f_1(\nabla u(x)) = \frac{f_0(x)}{det\ H u(x)}
\label{eq:mongemapere}
\end{equation}






\subsection{Formulation de J.D. Benamou et Y. Brenier}
Pour le moment, le coût ne dépend que des densités initiales et finale, cependant il semble pertinent de chercher à connaître ce qu'il se passe pendant le transport, cela nous amènera à prendre en compte la notion d'obstacles. 
Dans leur article de 1999, J.D. Benamou et Y. Brenier \cite{benamoubrenier}, ont donné une autre formulation au problème de transport optimal en réintroduisant le temps comme variable. Ils obtiennent alors une interprétation très simple du problème de transport optimal en terme de mécanique des fluides. 

\begin{theoreme}{Benamou, Brenier}
Soient $f_0$ et $f_1$ deux densités de probabilité assez régulières. Alors, 
\begin{align}
\min_{T\in\mathcal{T}(f_0,f_1)} \int\|x-T(x)\|^2dx =\min_{(f,v)\in\mathcal{C}_v}\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dtdx
\end{align}
Avec,
\begin{align*}
f(t,x)\ :&\ \RR\times\RR^n\rightarrow \RR \quad \text{la densité}\\
v(t,x)\ :&\ \RR\times\RR^n\rightarrow \RR^n \quad \text{champ de vecteurs vitesses}
\end{align*}
et 
\begin{align}
C_v=\left\{(f,v)\ |\ \frac{\partial f}{\partial t} + \text{div}_x (fv) =0,\ f(0,\cdot) = f_0,\ f(1,\cdot)=f_1 \right\}
\label{eq:contraintes}
\end{align}
\end{theoreme}
\begin{preuve}
Nous redonnons la preuve donnée par Benamou et Brenier en complétant les points qui nous semble obscurs.\\
Supposons $f_0$ et $f_1$ bornées et à support compact de $\RR^n$. Considérons $(f,v)$, suffisamment régulières, vérifiant \eqref{eq:contraintes}. Cette preuve repose sur l'utilisation des variables lagrangiennes $X(t,x)$ qui décrivent le comportement d'un fluide en "suivant" les particules. 




En termes formels, cela donne, 
\begin{align}
X(0,x)=x\quad\frac{\partial X}{\partial t}(t,x) = v(t,X(t,x))
\label{eq:defcoordlag}
\end{align}
Ainsi, pour toute fonction test $\phi$ :
\begin{align}
\int \phi(t,x)f(t,x)\ dxdt &=\int \phi(t,X(t,x))f_0(x)\ dxdt \label{eq:coordlagrangienn}\\ 
\int \phi(t,x)f(t,x)v(t,x)\ dxdt &= \int \frac{\partial X}{\partial t}(t,x)\phi(t,X(t,x))f_0(x)\ dxdt 
\end{align}
la première égalité vient du changement de variable $x=X(t,x)$, or, $f(t,X(t,x))$ décrit la densité de la particule de fluide déplacée le long du champ de vitesse $v$ à l'instant t, c'est donc la densité initiale en $x$, $f_0(x)$. \\

Remarquons que \eqref{eq:coordlagrangienn} et la condition $f(0,\cdot)=f_0,\ f(1,\cdot) = f_1$ impliquent que $T(x)=X(1,x)$ est un transport valide. En effet, pour tout borélien $B$, $\int_{T(x)\in B}f_0(x)\ dx = \int_{X(1,x)\in B}f_0(x)\ dx$ Ce qui signifie, qu'au temps $1$, la particule $x$ est dans le borélien $B$, comme la densité au temps $1$ est donnée par $f_1$ nous avons bien : $\int_{X(1,x)\in B}f_0(x)\ dx = \int_B f_1(x)\ dx$.


Puis remarquons, en prenant comme fonction test $\phi(t,x) = \|v(t,x)\|^2$ dans \eqref{eq:coordlagrangienn}, que: 
$$
\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2\ dxdt = \int_{\RR^n}\int_0^1 f_0(x)\|v(t,X(t,x))\|^2\ dxdt
$$
Nous obtenons, 
\begin{align*}
&= \int_{\RR^n}\int_0^1  f_0(x)\left\|\frac{\partial X}{\partial t} (t,x)\right\|^2dxdt \quad \text{par } \eqref{eq:defcoordlag}\\
&= \int_{\RR^n} f_0(x)\int_0^1 \left\|\frac{\partial X}{\partial t} (t,x)\right\|^2dtdx\\
&\text{par l'inégalité de Jensen : }\\
&\leq \int_{\RR^n} f_0(x)\left\|\int_0^1\frac{\partial X}{\partial t} (t,x) dt \right\|^2dx \\
&\leq \int_{\RR^n} f_0(x)\left\| X(1,x)-X(0,x) \right\|^2dx \\
&\leq \int_{\RR^n} f_0(x)\left\| X(1,x)-x \right\|^2dx  \quad \text{par } \eqref{eq:defcoordlag}\\
\end{align*}

Nous reconnaissons ici la forme qui apparaît dans la distance $L^2$ de Wasserstein, pour le transport $T(x) = X(1,x)$. Prenons $\nabla u(x)$ le transport optimal, nous avons alors :  
$$
\leq \int_{\RR^n} f_0(x)\left\| \nabla u(x)-x \right\|^2dx
$$
Finalement, nous avons montré que : 
$$
\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dxdt \leq \int_{\RR^n} f_0(x)\left\| \nabla u(x)-x \right\|^2dx = d(f_0,f_1)^2
$$
Il reste un trouver un couple $(f,v)$ tel que nous ayons l'égalité et la preuve sera complète. Choisissons $X(t,x) = x+t(\nabla u(x)-x)$, ce qui correspond à la paire (f,v) vérifiant pour toute fonction test $\phi$ : 
\begin{align*}
\int_{\RR^n} \int_0^1 \phi(t,x)f(t,x)\ dtdx &=\int_{\RR^n} \int_0^1 \phi(t, x+t(\nabla u(x)-x)) f_0(x)\ dxdt \\
\int_{\RR^n} \int_0^1 \phi (t,x)f(t,x)v(t,x)\ dtdx &= \int_{\RR^n} \int_0^1 \left(\nabla u (x) - x\right) \phi(t, x+t(\nabla u(x)-x)) f_0(x)\ dtdx
\end{align*}


Et en reprenant le raisonnement de la première étape, nous obtenons
\begin{align*}
\int_{\RR^n} \int_0^1 f(t,x)\|v(t,x)\|^2dtdx &= \int_{\RR^n} \int_0^1 f_0(x)\|v(t,X(t,x)\|^2\\
&= \int_{\RR^n} \int_0^1 f_0(x) \|\frac{\partial X(t,x)}{\partial t}(t,x)\|^2 dtdx \\
&= \int_{\RR^n} f_0(x) \|\nabla u(x) -x\|^2dx \\
\end{align*}
Ce qui achève la preuve. 
\end{preuve}

{\Huge à partir de la il est possible de recouvrer l'application de transport optimal}




\subsection{Conclusion}
Dans cette partie, nous avons introduit le problème de transport optimal. Démontrer l'existence et l'unicité du transport. 
Le problèem se reformule alors. 

Et nous avons montrer avec Benamou et Brenier que ce problème est équivalent au suivant : 

qui a une interprétation en terme de mécanique des fluides très pratiques. \\

Avant d'enatmmer la résolution numérique du problème de transport optimal, nous présentons les opérateurs proximaux qui joueront un rôle central dans notre modèle. 

\newpage
\section{Opérateurs proximaux}
\label{sec:prox}
L'opérateur proximal d'une fonction convexe généralise la notion de projection sur un ensemble convexe. Cette notion fût introduite pour répondre aux problèmes de minimisation de fonctions non nécessairement différentiables. 

Suivant la présentation de \emph{P.L. Combettes \emph{et} J.C. Pesquet} \cite{combettes} nous présentons une série de résultats sur les opérateurs proximaux, en complétant quelques preuves.
\paragraph{Notation :}Le domaine d'une fonction $f:\ \RR^n\longrightarrow\ ]-\infty,+\infty]$ est $\mathcal{D}(f) = \{ x\in\RR^n\ |\ f(x) < +\infty \} $. Notons $\Gamma_0(\RR^n)$ l'ensemble des fonctions convexes semi-continues inférieurement à valeurs dans $]-\infty,+\infty]$ telles que $\mathcal{D}(f) \neq \emptyset$. 

\begin{definition}{Sous - différentiel}
Soit $f\in\Gamma_0(\RR^n)$, le sous - différentiel de $f$ est l'application suivante,
$$
\fonction{\partial f}{\RR^n}{\mathcal{P}(\RR^n)}{x}{\left\{ u\in\RR^n\ |\ \forall y\in\RR^n,\ \langle u,y-x\rangle +f(x)\leq f(y) \right\}}
$$ 
Avec $\mathcal{P}(\RR^n)$, l'ensemble des parties de $\RR^n$.
\end{definition}


L'interprétation géométrique du sous-différentiel est la suivante. Il est formé par toutes les directions des hyperplan qui passent par le point $(x,f(x))$ et restent "sous" le graphe de la fonction $f$. \\

\exemple{Calculons le sous différentiel en tout point de 
$$
\fonction{f}{\RR}{\RR^+}{x}{|x|}
$$
Considérons deux cas, 
\begin{enumerate}
\item $\mathrm{x>0}$ (le cas $x<0$ est identique). Soit une direction $u$ appartient au sous différentiel de $f$ en x, alors $\forall y\in \RR$ : 
\begin{align*}
u(y-x) + f(x) \leq f(y)\\
u(y-x) + x \leq |y|\\
\end{align*}
En particulier, pour $y = 0$ nous obtenons $x(1-u) \leq 0$, ainsi $u\geq 1$. Et donc en prenant $y > x$ (en particulier, $y>0$) nous obtenons $u(y-x) + x \geq y$ ce qui est contraire à notre définition sauf si $u=1$. 
\item $\mathrm{x=0}$ Soit $u\in \partial f(0)$, alors $u$ doit vérifier pour tout $y\in \RR$,
\begin{align*}
uy\leq |y|
\end{align*}
En prenant $y>0$ nous obtenons que $u\leq 1$ et en prenant $y<0$, nous avons $u\geq -1$.
\end{enumerate}
En somme, 
$$
\partial f(x) =
\left\{
\begin{array}{cc}
\{-1\} & x <0 \\
\left[-1,1\right] & x =0\\
\{1\} & x>0\\
\end{array}
\right.
$$
Ce qui correspond bien à l'interprétation géométrique donnée et à la figure \ref{fig:subgradient}.
\begin{figure}[!h]
\centering
\includegraphics[scale=0.6]{img/sub_gradient.png}
\caption{\label{fig:subgradient}Illustration du sous - différentiel de $|x|$ en $0$}
\end{figure}
}

\begin{propriete}
Soit $f\in\Gamma_0(\RR^n)$. Si $f$ est différentiable sur son domaine, alors 
$$
\forall x\in \mathcal{D}(f)^{\circ},\ \partial f(x) = \{\nabla f(x)\}
$$
\end{propriete}
\begin{preuve}
Comme une $f$ est convexe différentiable, nous avons la caractérisation suivante du gradient : 
$$
\forall x,y\in\RR^n,\quad \langle \nabla f (x) ,y-x\rangle +f(x) \leq f(y)
$$
qui implique que $\nabla f(x) \in\partial f(x)$.\\
Supposons à présent que $p\in\partial f(x)$. Soit une suite $y_n^1$ de $\RR^n$ qui tend vers x. Écrivons, $y_n^1=x+k_n$ avec $\|k_n\|\rightarrow 0$. Posons alors $y_n^2$ une seconde suite de $\RR^n$ telle que $y_n^2= x-k_n$, alors : 
$$
\left\{
\begin{array}{cc}
\langle p, k_n\rangle &\leq f(x +k_n) - f(x) \\
\langle p,-k_n\rangle & \leq f(x-k_n)-f(x) \\
\end{array}
\right.
$$
Comme $f$ est différentiable dans son domaine, nous pouvons écrire, 
$$
f(x\pm k_n)=f(x) \pm \langle \nabla f(x), k_n\rangle
$$
Ainsi, nous obtenons
$$
\left\{
\begin{array}{cc}
\langle p, k_n\rangle &\leq \langle \nabla f(x), k_n\rangle \\
\langle p, k_n\rangle &\geq \langle \nabla f(x), k_n\rangle \\
\end{array}
\right.
$$
Ainsi $\langle p,k_n \rangle = \langle \nabla f(x),k_n\rangle$. En faisant tendre $n$ vers $+\infty$, comme la suite $k_n$ converge fortement vers $0$, elle va aussi converger faiblement. Ce qui implique que $p=\nabla f(x)$. 
\end{preuve}
Cette propriété est cohérente avec l'exemple précédent, $\partial f(x) =f'(x)$ partout où, $f$ est différentiable, c'est à dire $x\neq 0$. 

\begin{propriete}
Si $f = \alpha f_1+\beta f_2$, avec $f_1,\ f_2\in\Gamma_0(\RR^n)$ et $\alpha,\ \beta \geq 0$. Alors 
\begin{align*}
\partial f(x) &= \alpha \partial f_1(x) + \beta \partial f_2(x) \\
&= \left\{u\in\RR^n\ |\ \exists (u_1,u_2)\in \partial f_1(x)\times f_2(x),\ u = \alpha u_1 + \beta u_2 \right\}
\end{align*}
\end{propriete}
\begin{preuve}
\vspace{-1cm}
\paragraph{$\alpha\partial f_1(x) +\beta \partial f_2(x) \subset
\partial (\alpha f_1 +\beta f_2)(x)$ :}$u\in\RR^n$ est dans $\partial (\alpha f_1 +\beta f_2)(x)$ si et seulement si pour tout $y\in \RR^n$, nous avons 
$$
\langle u,y-x\rangle + \alpha f_1(x) +\beta f_2(x) \leq  \alpha f_1(y) +\beta f_2(y) 
$$
En particulier en prenant $u\in \alpha\partial f_1(x) +\beta \partial f_2(x) $, au sens donné dans la propriété, cette inégalité est vérifiée. 

\paragraph{$\alpha\partial f_1(x) +\beta \partial f_2(x) \supset
\partial (\alpha f_1 +\beta f_2)(x)$ :}Soit $u\in\partial (\alpha f_1 +\beta f_2)(x)$, par contradiction, supposons que  $u\notin \alpha\partial f_1(x) +\beta \partial f_2(x)$, c'est à dire : 
$$
\forall (u_1,u_2)\in \partial f_1(x) \times \partial f_2(x),\quad u\neq \alpha u_1+\beta u_2
$$
{\Huge EMPTY}
\end{preuve}
\exemple{Calculons le sous différentielle de $f(x)+\frac{1}{2}\|x-y\|^2$, pour une certaine fonction $f\in\Gamma_0(\RR^n)$ et $y\in_RR^n$ fixé. 
}

La notion de sous - différentiel est centrale car il permet de caractériser les minimiseurs d'une fonctions (même non différentiable).
\begin{theoreme}{}
Soit $f\in\Gamma_0(\RR^n)$. Le point $x^{\star}\in\RR^n$ est un point de minimum global de $f$ si et seulement si 
$$
0\in \partial f(x^{\star})
$$ 
\end{theoreme}
\begin{preuve}
\vspace{-1cm}
Soit $x^{\star}$ le minimum global de $f$, 

\begin{align*}
\Longleftrightarrow \quad & \forall y\in\RR^n\quad  f(x^{\star})                        \leq f(y)\\
\Longleftrightarrow \quad &                  \langle 0,y-x\rangle +f(x^{\star})  \leq f(y)\\
\Longleftrightarrow \quad &                   0\in \partial f(x^{\star})       
\end{align*}

\end{preuve}

L'opérateur proximal est un outil important dans l'optimisation de fonctions convexes non différentiables. C'est une application minimisant une fonction fortement convexe dépendant de la fonction originale $f$. En trouvant un compromis entre atteindre le minimum de la fonction non différentiable $f$ et en forçant une proximité avec l'argument, nous approchons le minimum de $f$. 

\begin{definition}{Opérateur proximal}
Soit $f\in\Gamma_0(\RR^n)$. L'opérateur proximal de $f$ est noté $\prox_f$ et il est défini pour tout $x\in\RR^n$ par : 
$$
\prox_f(x) = \argmin_{y\in\RR^n} f(y) +\frac{1}{2}\|x-y\|^2
$$
\end{definition}
\exemple{Prenons, l'indicatrice d'un ensemble convexe non vide $C$ : 
$$
\mathrm{i}_C = \left\{
\begin{array}{cc}
0 & x\in C\\
+\infty & x \notin C
\end{array}
\right.
$$
Alors, 
\begin{align*}
prox_{\mathrm{i}_C}(x) &= \arg \min_{y\in\RR^n} \mathrm{i}_C(y) + \frac{1}{2}\|x-y\|^2\\
&= \arg\min_{y\in C} \frac{1}{2}\|x-y\|^2\\
&= \mathrm{P}_C (x)\\ 
\end{align*}
Où, $\mathrm{P}_C$ désigne la projection orthogonale sur $C$. 
}\\
Sur la figure \ref{fig:proximal} (tirée de \cite{parikh2014proximal}), nous avons en \textbf{gras} la frontière de la fonction $f$, les lignes fines sont les lignes de niveaux de $f$. Nous évaluons $\prox_f$ sur les points bleus les associent aux points rouges correspondant. Les trois points dans le domaine restent dans le domaine et se déplacent vers le minimum, et les deux à l'extérieur du domaine se placent sur la frontière de celui-ci en direction du minimum. 
\begin{figure}[!h]
\centering
\includegraphics[scale=0.7]{img/proximal.png}
\caption{\label{fig:proximal}Illustration de l'opérateur proximal}
\end{figure}
Un scalaire $\gamma >0 $ peut être introduit dans cette définition $prox_{\gamma f}$, ce paramètre contrôle le poids apporté à la minimisation de $f$. Un $\gamma$ grand, forcera de se rapprocher fortement du minimum de $f$, alors qu'un $\gamma$ petit favorisera les points proches de $x$. Le résultat important est le suivant. 
\begin{propriete}
Soit une fonction $f\in\Gamma_0(\RR^n)$. Alors, $\prox_f(x)$ existe et est unique pour tout $x\in\RR^n$.\\
De plus, il est caractérisé par : 
$$
\forall (x,p)\in \RR^n\times\RR^n\quad p=\prox_fx\quad\Longleftrightarrow\quad x-p\in\partial f(p)
$$
En particulier, si $f$ est différentiable alors : 
$$
\forall (x,p)\in \RR^n\times\RR^n\quad p=\prox_fx\quad \Longleftrightarrow \quad x-p=\nabla f(p)
$$
\end{propriete}
\begin{preuve}
$\forall x \in \RR^n$, la fonction $g\ :\ y \rightarrow f(y) +\frac{1}{2}\|x-y\|^2$ est strictement convexe et dans $\Gamma_0(\RR^n)$ (i.e. non identiquement égale à $+\infty$). Ce qui montre que le minimum de $g$ est toujours atteint et est unique. 


\paragraph{$\Leftarrow$} Supposons que $x-p\in\partial f(p)$. Alors, pour tout $y\in\RR^n$, nous avons 
$$
\langle x-p,y-p\rangle +f(p) \leq f(y)
$$
Or,
\begin{align*}
\langle x-p,y-p\rangle &= \frac{1}{2}\langle x-p,y-x+x-p\rangle + \frac{1}{2}\langle  x-y+y-p,y-p\rangle\\
&=\frac{1}{2} \|x-p\|^2 + \frac{1}{2}\|y-p \|^2 +\frac{1}{2}\left( \langle x-p,y-x\rangle + \langle x-y,y-p\rangle\right)\\
&= \frac{1}{2} \|x-p\|^2 + \frac{1}{2}\|y-p \|^2 -\frac{1}{2}\|x-y\|^2
\end{align*}
Ainsi,
\begin{align*}
\frac{1}{2} \|x-p\|^2 + \frac{1}{2}\|y-p \|^2 -\frac{1}{2}\|x-y\|^2 + f(p) &\leq f(y) \\
\frac{1}{2} \|x-p\|^2 + f(p) &\leq f(y) + \frac{1}{2}\|x-y\|^2\\
\end{align*}
Donc $p=prox_f(x)$.
\paragraph{$\Rightarrow$} Soit $(x,p)\in \RR^n\times\RR^n$ telle que $p=prox_f(x)$. Alors $\forall y\in\RR^n$,
{\Huge je me demande si c'est pas eja prouvé}
\end{preuve}
 
De plus, 
\begin{propriete}
Le point $x^{\star}$ minimise $f$ si et seulement si 
$$
x^{\star} = \prox_f(x^{\star})
$$
\end{propriete} 
\begin{preuve}
\vspace{-1cm}
\paragraph{$\Rightarrow$} Supposons que $x^{\star}$ minimise $f$ alors, $\forall y\in\RR^2$,
$$
f(x^{\star}) +\frac{1}{2}\|x^{\star}-x^{\star}\|^2\leq f(y)+\frac{1}{2}\|x^{\star}-y\|^2
$$
Donc, $\prox_f(x^{\star}) = x^{\star}$.
\paragraph{$\Leftarrow$} Utilisons la caractérisation de minimisation donnée par le sous - différentiel. Le point $\tilde{x}$ minimise $f(x) +\frac{1}{2}\|x-y\|^2$, c'est à dire $\tilde{x} = \prox_f(y)$, si et seulement si 
$$
0\in \partial f(\tilde{x}) + (\tilde{x}-y)
$$
Ou la somme est entendu au sens de la somme entre un ensemble et un singleton, comme dans l'exemple précédent. \\
En prenant $x^{\star} = y=\tilde{x}$, nous obtenons $0\in\partial f(x^{\star})$. Et donc $x^{\star}$ minimise $f$.
\end{preuve}

Cela suggère des algorithmes de point fixes. Cela fonctionnerait si $\prox_f$ était une contraction (i.e. c - lipschitzienne, $0<c<1$). Ce n'est pas le cas, cependant nous avons une propriété de non expansivité qui est suffisante pour une itération de point fixe : 
$$
\|prox_f(x)-prox_f(y)\|^2\leq \langle prox_f(x)-prox_f(y),x-y\rangle
$$
C'est un cas particulier d'opérateur non expansif (comme par exemple, $-Id$ ou les rotations, \cite{browder1965nonexpansive,brezis1973ope}). 
\begin{definition}{Application non expansive}
Une application $N:\ \RR^n\rightarrow \RR^n$ est dite non expansive si 
$$
\forall (x,y)\in\RR^n\times\RR^n,\quad \|N(x)-N(y)\| \leq \|x-y\|
$$
C'est à dire que $N$ est 1-lipschitzienne. Nous avons la caractérisation suivante, $N$ est non expansive si et seulement si 
$$
\forall (x,y)\in\RR^n\times\RR^n,\quad \|N(x)-N(y)\|^2 \leq  \langle N(x)-N(y),x-y\rangle
$$
\end{definition}

Si $N$ est un opérateur non expansif alors $\forall \alpha \in ]0,1[$ l'opérateur $T = (1-\alpha)I + \alpha N$ a les mêmes poinst fixes que $N$, et qu'en itérant $T$, il y a convergence vers les poinst fixes. C'est à dire, la suite 
$$
x_{n+1} = (1-\alpha )x_n +N(x_n)
$$
converge vers un point fixe de $N$.





 
Soit le problème de minimisation: \\

\fbox{
  \parbox{\textwidth}{
  Soient $f_1$ et $f_2$ deux fonctions de $\Gamma_0(\RR^n)$ telles que $f_1+f_2\in\Gamma_0(\RR^n)$, en particulier le support de $f_1+f_2$ est non vide. Supposons que
  $$
  \lim_{\|x\|\rightarrow+\infty} f_1(x)+f_2(x) =+\infty
  $$
Considérons alors le problème de minimisation, 
Trouver $x\in\RR^n$ qui réalise le 
\begin{align}
\min_{x\in\RR^n}f_1(x)+f_2(x)
\label{eq:pbmminimi}
\end{align}
  }
}

\begin{propriete}
Le problème \ref{eq:pbmminimi} admet une solution et pour tout $\gamma >0$, elle est caractérisée par :
$$
\left\{
\begin{array}{rcl}
x^{\star} &= & \prox_{\gamma f_2} y \\
\prox_{\gamma f_2}y &= & \prox_{\gamma f_1}(2\prox_{\gamma f_2}y -y)
\end{array}
\right.
$$
\end{propriete}

\begin{preuve}
Omettons le paramètre $\gamma$ qui ne change rien à la preuve. $f_1+f_2$ est une fonction convexe, infinie à l'infini donc l'existence d'un minimiseur est immédiate.
\begin{align*}
x^{\star} =\argmin_x f_1(x) + f_2(x) &\Longleftrightarrow 0\in \partial (f_1+f_2)(x^{\star}) \\
&\Longleftrightarrow \exists (x_1,x_2)\in \partial(f_1)(x^{\star})\times\partial(f_2)(x^{\star}) \text{ telle que } 0=x_1+x_2
\end{align*}
Comme le support de $f_1+f_2$ est non vide, $\exists\ y$ tel que $x^{\star}-y\in \partial(f_1)(x)$ et $y-x^{\star}\in\partial(f_2)(x)$.
\begin{align*}
\Longleftrightarrow x^{\star}=prox_{f_2} (y) \text{ et } &(2x^{\star}-y)-x^{\star}\in\partial(f_1)(x^{\star})\\
&\Longleftrightarrow x^{\star} = prox_{f_1}(2x^{\star}-y)
\end{align*}
Ce qui termine la preuve. Cependant, pour la forme algorithmique, développons un peu les calculs : 


\end{preuve}

Suivant les idées d'algorithmes de point fixes sur les opérateurs proximaux, nous aintroduisons l'algorithme suivant dit de \emph{Douglas - Rachford}.
\begin{algorithm}
\caption{Douglas-Rachford}
\begin{algorithmic}
\STATE Soit $\epsilon \in ]0,1[,\ \gamma >0,\ y_0\in \RR^n$
\FOR{$n=0,\ 1,\ \ldots$}
\STATE $x_n = \prox_{\gamma f_2} y_n$
\STATE $\lambda_n \in [\epsilon,2-\epsilon]$
\STATE $y_{n+1} = y_n + \lambda_n(\prox_{\gamma f_1} (2x_n-y_n)-x_n)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{propriete}
Toute suite $(x_n)$ générée par l'algorithme de Douglas - Rachford converge vers une solution du problème \ref{eq:pbmminimi}.
\end{propriete}








\newpage

\section{Méthode numérique}
\label{sec:numerique}
Dans cette section, nous présentons différentes manières d'approcher numériquement la solution du problème \eqref{eq:MP}. La méthode consiste principalement à trouver une manière efficace de calculer les contraintes. \\

La présentation de cette section part de l'algorithme de Douglas - Rachtford. \\

Dans cette section nous nous plaçons dans un domaine spatial de dimension 1. La méthode présentée s'étend sans grande difficultés en dimensions supérieures. 


\subsection{Sur des grilles centrées}
Le problème continu {\Huge Insérer ref} est approximé par 
\begin{align}
\min_{w\in\mathcal{G}} J(w) + \iota_C(w)
\end{align}
Où, $\mathcal{G}$ est la grille de discrétisation du carré espace temps. 

\begin{figure}[!h]
\centering
\includegraphics[width=0.8\linewidth]{img/grille_centree.png}
\caption{•}
\end{figure}

$J$ est la version discrète de la fonctionnelle
$$
é
$$


L'équation de conservation des contrainte \eqref{eq:contraintes}, suggère une discrétisation différences finies du domaine. 



\subsection{Avec des grilles décentrées}
\section{Implémentation}

\section{Exemples}

\subsection{Opérateur proximal de $J$}
La fonctionnelle $J$ est simple dans le sens où son opérateur proximal peut être calculé dans une forme fermée. 
\begin{proposition}
\begin{align*}
\forall w\in\mathcal{G},\ \prox_{\gamma J}(w) = \left( \prox_{\gamma J}(w_k)\right)_{k\in\mathcal{G}}
\end{align*}
Où, pour tout $w_k=(m_k,f_k)\in\RR^n\times\RR$, 
$$
\prox_{\gamma J}(m_k,f_k) =\left\{
\begin{array}{cl}
\left(\frac{f^{\star}_k m_k}{f^{\star}_k+\gamma} ,f^{\star}_k\right) & \text{ si } f^{\star}_k >0\\
(0,0) & \text{ sinon }
\end{array}\right.
$$
et $f^{\star}_k$ est la plus grande racine réelle du polynôme de degré 3 : 
\begin{align}
P(X) = (X-f_k)(X+\gamma)^2 -\frac{\gamma}{2}\|m_k\|^2=0
\end{align}
\end{proposition}
\begin{preuve}
Posons $(m,f)=\prox_{\gamma J}(m_k,f_k)$. Si $f>0$, comme $J$ est $C^1$ et strictement convexe sur $\RR^n\times\RR^{+,\star}$, nécessairement, $(m,f)$ est l'unique solution de $\nabla J(m,f)=0$, ce qui donne 
$$
\left\{
\begin{array}{cc}
\gamma \frac{m}{f}+m-m_k &= 0\\
-\gamma \frac{\|m\|^2}{f^2} + f-f_k &= 0
\end{array}
\right.
$$
En réarrangeant ces equations, nous obtenons bien que $P(f) = 0$ et $m=\frac{f m}{f+\gamma}$. Ce qui montre au passage que si $P$ possède au moins une racine réelle strictement positive $f^{\star}_k$, alors elle est nécessairement unique $f^{\star}_k =f $. \\
Si ce n'est pas le cas, alors nous avons nécessairement $f= 0$ et par définition de $J$, $m=0$.
\end{preuve}
\newpage

\section{Pbm de transport optimal}
Soient deux densités $f_0$ et $f_1$ de même masse, suffisamment régulière, sur un domaine $[0,1]^d$.\\
Un transport de $f_0$ sur $f_1$ est une application $T$ telle que :
$$
f_0(x)=f_1(T(x))|det(\partial T(x))|
$$
Le transport optimal est celui qui minimise le coût $\int c(x,T(x)dx$ parmi tous les transports de $f_0$ sur $f_1$. Dans notre problème : $C(x,y) =\|x-y\|^2$. 

\section{Formulation de Benamou et Brenier}
On montre que la géodésique entre $f_0$ et $f_1$ est : 
$$
f(x,t)=f_0((1-t)Id+tT(x))|det((1-t)Id+t\partial T(x))|
$$
et que ce chemin minimise le problème suivant : 
$$
\min_{(f,v)\in C_v} \int_{[0,1]^d}\int_0^1 f(x,t)\|v(x,t)\|^2dtdx
$$

Où : $C_v= \{(f,v)|\partial_t f+div_x(v)=0;v(0,.)=0,v(1,.)=0,f(.,0)=f_0,f(.,1)=f_1\}$

On pose $m = fv$ pour obtenir le problème de minimisation 
$$
\min_{(f,m)\in C}J(m,f)=\int_{[0,1]^d}\int_0^1 \theta (m,f)dtdx
$$
avec $\theta (m,f) = \frac{\|m\|^2}{f} si f>0 \quad 0 si (m,f)=(0,0)\quad \infty sinon$

Rq : on a ramené le pbm de transport optimal à celui de trouver la géodésique entre les deux densités. 
\section{Méthodes numériques}
On présente le cas 1D qui nous a accaparé. \\
Carré espace temps : $[0,1]^2$ que l'on discrétise : 
$$
G_c = \{(x_i=\frac{i}{N},t_j=\frac{j}{P})|0\leq i\leq N,\ 0\leq j\leq P\}
$$
On notre $E_c =(\RR^2)^{G_c}$ l'espace des variables centrées et $ V=(m_{ij},f_{ij}) \quad 0\leq i\leq N,\ 0\leq j\leq P$ les variables discrétisées. \\

Dans le but de capturer l'équation de continuité, on introduit une grille décentrée : 
$$
G_s^x = \{(x_i=\frac{i+1/2}{N},t_j=\frac{j}{P})|-1\leq i\leq N,\ 0\leq j\leq P\}
$$
et 
$$
G_s^t = \{(x_i=\frac{i}{N},t_j=\frac{j+1/2}{P})|0\leq i\leq N,\ -1\leq j\leq P\}
$$

On note $E_s=\RR^{G_s^x}\times\RR^{G_s^t}$ l'espace des variables décentrées, et $U=(\bar{m_{ij}}\quad 1\leq i\leq N,\ 0\leq j\leq P,\bar{f_{ij}},\quad 0\leq i\leq N,\ -1\leq j\leq P$



\section{Opérateurs}
On introduit plusieurs opérateurs pour lier les variables décentrées et les variables centrées.
\subsection{Interpolation}
$$
I:E_s \rightarrow E_c
$$
tq 
$$
m_{ij} = = (\bar{m}_{i+1/2,j}+\bar{m}_{i-1/2,j})/2.
$$
et
$$
f_{ij} = = (\bar{f}_{i,j+1/2}+\bar{f}_{i,j-1/2})/2.
$$
Cet opérateur s'interprète matriciellement : 
$$
m = \bar{m}I_m
$$
et 
$$
f = I_f\bar{f}
$$

\subsection{Divergence}
l'opérateur qui approxime la divergence
$$
div : E_s\rightarrow \RR^{G_c}
$$
et 
$$
div(\bar{m},\bar{f})_{ij} = (\bar{m}_{i+1/2,j}-\bar{m}_{i-1/2,j}) + (\bar{f}_{i,j+1/2}-\bar{f}_{i,j-1/2})
$$

\subsection{Frontières}
Un opérateur pour extraire les frontières : 

$$
b(\bar{m},\bar{f}) = ((\bar{m}_{-1/2,j},\bar{m}_{N+1/2,j});(\bar{f}_{i,-1/2},\bar{f}_{i,P+1/2}))
$$
et on impose les conditions aux frontières suivantes : 
$$
b(\bar{m},\bar{f}) = b_0=(0,0,f_0,f_1);
$$

\subsection{Problème discrétisé}

On a le problème suivant 

$$
\min_{U=(\bar{m},\bar{f})\in E_s} \theta(I(\bar{m},\bar{f})) + \iota_C(U)
$$
avec l'ensemble des contraintes :
$$
C=\{(\bar{m},\bar{f})\in E_s|\ div(\bar{m},\bar{f}) = 0\ b(\bar{m},\bar{f}) = b_0 \}
$$

\section{Résolution par algorithme de séparation de proximité}
Ces algorithmes sont des généralisation des algos de gradient conjugués. \\

\textbf{Remarque : } On a $J(m,f)=\int_{[0,1]^d}\int_0^1 \frac{\|m\|^2}{f} dtdx$ donc si $f\rightarrow\infty$ on a $J\rightarrow 0$ donc ce n'est pas coercif ce qui rend l'existence de minimiseurs non triviale. Et si $f\rightarrow 0$, on a $j\rightarrow \infty$ donc les méthodes de gradient conjugués ne peuvent pas s'appliquer, le gradient n'est pas lipschitz. \\

On veut résoudre le pbm suivant : 
$$
\min_{z=(U,V)\in E_s\times E_c} G_1(z)+G_2(z)
$$
où, $G_1(z) = J(U)+\iota_C(U)$ et $G_2(z)=\iota_{C_s}(z)$ et $C_s=\{z=(U,V)\in E_s\times E_c\ | \ V=I(U) \}$


\remarque{$G_1$ est la fonctionnelle originelle et $G_2$ vient de notre introduction des variables décalées. }\\

On va alors calculer les opérateurs de proximités de $G_1$ et $G_2$.On dit que $G_1$ est \textbf{simple} dans la mesure ou : 

$$
Prox_{\gamma G_1} (U,V) = (Prox_{\gamma C} (U),Prox_{\gamma J} (V))
$$

\subsection{Opérateur de proximité de $G_2$}

$$
Prox_{\gamma C_s} (U,V) = arg\min_{z'\in C_s} \frac{1}{2}\|z-z'\|^2= Proj_{C_s}(U,V)
$$
Suivant l'article de Papadakis, Peyré et Oudet on a :

$$
Prox_{\gamma C_s} (U,V)  = (\tilde{U},I(\tilde{U}))
$$
et 

$$
\tilde{U} = (Id +I^{\star}I)^{-1}(U+I^{\star}(V))
$$
Ou $I^{\star}$ est l'adjointe de $I$. ce qui nous donne en terme de matrice : 
$$
\tilde{\bar{m}} = (\bar{m}+mI_m^{\star})(Id +I_mI_m^{\star})^{-1}
$$
et 
$$
\tilde{\bar{f}} = (Id +I_f^{\star}I_f)^{-1}(\bar{f}+I_f^{\star}f)
$$
enfin : $\tilde{m}=\tilde{\bar{m}}I_m$ et $\tilde{f}=I_f\tilde{\bar{f}}$

Comme l'opérateur d'interpolation s'interprète matriciellement, son adjoint est donné par la transposée. 

\subsection{Opérateur de proximité de $J$}

$Prox_{\gamma J}(V)=(Prox_{\gamma J}(V_k))_{k\in G_c}$ et 

$$
Prox_{\gamma J}(m_k,f_k) = (\frac{f^{\star}_km_k}{f^{\star}_k+2\gamma},f^{\star}_k)\ si\ f^{\star}>0 \qquad (0,0)\ sinon
$$ 
et $f^{\star}_k$ est la plus grande racine réelle du polynôme de degré 3 donné par : 
$$
P(x) = (X-f_k)(X+2\gamma)^2-\gamma \|m_k\|^2
$$
La racine est calculée rapidement avec l’algorithme de Newton. 

\subsection{Opérateur de proximité de $\iota_C$}

$$
Prox_{\gamma C} = Proj_C
$$
on réécrit l'ensemble des contraintes sous la forme : 

$$
C=\{U\in E_s\ | \ AU=y\} \qquad A=(div,b)^T \qquad y = (0,b0)^T
$$

et on obtient $Proj_C(U)=(Id +A^{\star}\Delta^{-1}A)U + A^{\star} \Delta^{-1}y$ avec $\Delta^{-1}=(A^{\star}A)^{-1}$ et son problème requiert la résolution d'un problème de poisson. 

\section{Algorithme de Douglas Rachford}
On procède à l'itération suivante : $(z^l,w^l)\in (E_s\times E_c)\times (E_s\times E_c)$ avec un $w^0$ donné : 
$$
z^{l+1}=Prox_{\gamma G_2} (w^l)
$$
et 
$$
w^{l+1}=(1-\frac{\alpha}{2})w^l + \frac{\alpha}{2}RProx_{\gamma G_2}\circ RProx_{\gamma G_1} (w^l)
$$

Où on a posé : $RProx_{\gamma G} = 2Prox_{\gamma G}-Id$.\\
On montre alors que pour $\alpha\in ]0,2[$ et $\gamma>0$ la méthode converge $z^l\rightarrow z^{\star}$ ce qui permet de retrouver la géodésique, car : 
$$
z^l=(U^l,V^l)\rightarrow (U^{\star},V^{\star}) \qquad V^{\star}=(m^{\star},f^{\star})
$$





\newpage
\bibliographystyle{plain}
\bibliography{biblio.bib}
\newpage






\begin{appendices}
\section{Calcul des variations}
\label{sec:variations}
Rapellons ici la méthode directe du calcul des variations qui nous servira pour notre démonstration.
 
\begin{definition}{Fonction continue inférieurement}
Dans un espace métrique $\mathcal{X}$, une fonction $f\ :\ \mathcal{X}\rightarrow \RR \cup \{+\infty\}$ est dite semi-continue inférieurement si pour toute suite $x_n\rightarrow x$
$$
f(x) \leq \liminf_n f(x_n)
$$
\end{definition}
\begin{definition}{Espace compact}
Un espace métrique $\mathcal{X}$ est dit compact si de toute suite $(x_n)$ il existe une sous suite convergente $x_{n_k} \rightarrow x\in\mathcal{X}$. 
\end{definition}

\begin{theoreme}{Weierstrass}
\label{thm:weierstrass}
Si $f\ :\ \mathcal{X}\rightarrow \RR \cup \{+\infty\}$ est semi-continue inférieurement, et $\mathcal{X}$ est un espace compact. Alors, il existe $\bar{x}\in\mathcal{X}$ tel que $f(\bar{x}) = \left\{f(x)\ |\ x\in\mathcal{X}\right\}$
\end{theoreme}
\begin{preuve}
Soit $l := \inf\left\{f(x)\ |\ x\in\mathcal{X}\right\}\in\RR\cup\{-\infty\}$. ($l=+\infty$ si et seulement si $f$ est identiquement égale à $+\infty$ auquel cas, tout point de $\mathcal{X}$ minimise $f$. Par définition de l'infimum, il existe une suite $(x_n)\in\mathcal{X}$ telle que telle que $f(x_n) \rightarrow l$. Quitte à extraire une sous suite, on peut supposer que $x_n\rightarrow \bar{x}$.
Par semi-continuité inférieure, nous avons $f(\bar{x}) \leq\liminf_n f(x_n)= l$. D'autre part, $f(\bar{x}) \geq l$ puique $l$ est l'infimum. \\
Ainsi, $l=f(\bar{x})\in\RR$.
\end{preuve}
\begin{definition}{Convergence faible et faible-*}
Une suite $x_n$ d'un espace de Banach $\mathcal{X}$ converge faiblement vers $x$, que nous notons $x_n \rightharpoonup x$, si pour tout $\xi\in\mathcal{X}'$ (où, $\mathcal{X}'$ est le dual topologique de $\mathcal{X}$ et $\langle \cdot,\cdot \rangle$ désigne le produit de dualité entre ces deux espaces), nous avons $\langle x_n,\xi\rangle \rightarrow \langle x,\xi\rangle$. \\

Une suite $\xi_n\in\mathcal{X}'$ converge faiblement-* vers $\xi$, que nous notons $\xi_n \overset{\ast}{\rightharpoonup}\xi$, si pour tout $x\in \mathcal{X}$ nous avons $\langle
x,\xi_n\rangle\rightarrow\langle x,\xi\rangle$.

\end{definition}

\begin{theoreme}{Banach - Alaoglu}
Si $\mathcal{X}$ est séparable et $\xi_n$ est une suite bornée de $\mathcal{X}'$. Alors il existe une sous suite $\xi_{n_k}$qui converge faiblement-* vers un $\xi\in\mathcal{X}'$.  
\end{theoreme}

\begin{definition}{Mesures de Radon}
Une mesure $\lambda$ sur la tribu borélienne d'un espace $\Omega$ séparable est dite mesure de Radon si 
\begin{enumerate}
\item $\lambda$ est intérieurement régulière : $\forall B\in\mathcal{B}(\Omega), \lambda (B) = \sup_{K\subset B, K \text{ compact}} \lambda (K)$,
\item $\lambda$ est localement finie : pour tout point $x$ il existe un voisinage $B$ tel que $\lambda (B) < +\infty$. 
\end{enumerate}
Notons $\mathcal{M}(\Omega)$ l'ensemble des mesures de Radon sur $\Omega$. Pour ces mesures, il est possible d'associée une mesure positive $|\lambda |\in \mathcal{M}_+(\Omega)$ : 
$$
|\lambda |(B) = \sup \left\{ \sum_i |\lambda (B_i)| \quad B = \bigcup_i B_i \ B_i\cap B_j = \emptyset \ i\neq j \right\}
$$
\end{definition}

\begin{theoreme}{Riesz pour les mesures de Radon}
Soient $\Omega$ une espace séparable et localement compact, $\mathcal{X} = C_0(\Omega)$ l'espace des fonctions continues qui s'annulent à l'infini, muni de la norme $\sup$. \\
Alors, tout élément de $\mathcal{X}'$ est représenté de façon unique par un élément de $\mathcal{M}(\Omega)$. C'est à dire, $\forall \xi\in\mathcal{X}'$il existe un unique $\lambda\in\mathcal{M}(\Omega)$ tel que $\langle \phi,\xi \rangle =\int \phi d\lambda\quad\forall\phi\in\mathcal{X}$. \\
De plus $\mathcal{X}'$ est isomorphe $\mathcal{M}(\Omega)$ muni de la norme $\|\lambda\|=|\lambda|(\Omega)$. 
\end{theoreme}

Pour les mesures de Radon de $\mathcal{M}(\Omega)$, la converge faible-* devrait être celle de la dualité avec $C_0(\Omega)$. Cependant, une autre notion de convergence est intéressante, celle avec la dualité de $C_b(\Omega)$, les fonctions continues bornées sur $\Omega$. Par abus de notations, nous la noterons aussi $\rightharpoonup$ : $\lambda_n\rightharpoonup\lambda$ si et seulement si pour toute fonction $\phi\in C_b(\Omega)$ nous avons $\int \phi d\lambda_n=\int \phi d\lambda$. Remarquons que si $\Omega$ est compact, $C(\Omega)=C_0(\Omega)=C_b(\Omega)$ et ces notions de convergences sont alors égales. \\

Les mesures de probabilités sur $\Omega$, $\mathcal{P}(\Omega)$, sont un sous-ensemble particulier de $\mathcal{M}(\Omega)$. $\mu\in\mathcal{P}(\Omega) \Leftrightarrow \mu \in\mathcal{M}_+(\Omega)$ et $\mu(\Omega) = 1$, dans ce cas, $\mu$ et $|\mu |$ coïncident. 
\begin{definition}{Tension d'une suite de mesure de probabilité}
Une suite $\mu_n$ de probabilité sur $_Omega$ est dite tendue si pour tout $\epsilon>0$ il existe un sous-ensemble compact $K\subset\Omega$ tel que $\mu_n(\Omega\backslash K) <\epsilon$ pour tout $n$. 
\end{definition}
\begin{theoreme}{Prokhorov}
\label{thm:prokhorov}
Soit $\mu_n$ une suite tendue de mesure de probabilité sur un espace métrique complet et séparable $\Omega$. Alors il existe une de probabilité sur $\Omega$, $\mu$, et une sous -suite $\mu_{n_k}$ telle que $\mu_{n_k} \rightharpoonup \mu$ (dans la dualité avec $C_b(\Omega)$). \\
Réciproquement, toute suite $\mu_n\rightharpoonup \mu$ est nécessairement tendue.  
\end{theoreme}

Rappelons le résultat principal concernant la compacité dans l'espace des fonctions continues. 
\begin{theoreme}{Ascoli - Arzelà}
\label{thm:ascoli}
Si $\Omega$ est un espace métrique compact et $f_n\ :\ \Omega\rightarrow\RR$ est équicontinue et équiborné. Alors il existe une sous suite $f_{n_k}$ qui converge uniformément vers une fonction continue $f\ :\ \Omega\rightarrow\RR$. \\
Réciproquement, un sous-ensemble de $C(\Omega)$ est relativement compact pour la convergence uniforme si et seulement si tout ses éléments sont équicontinus et équibornés.
\end{theoreme}











\end{appendices}


\newpage
\addcontentsline{toc}{section}{\listfigurename}
\listoffigures
\addcontentsline{toc}{section}{\lstlistlistingname}
\lstlistoflistings


\end{document}
