\documentclass[a4paper,12pt]{article}

\input{modele_documents.tex}

\setlength{\parindent}{0pt}
\newcommand\titre{OT}
\newcommand\auteur{Timothée \textsc{Schmoderer}}
\newcommand\dateDoc{2017/2018}
\newcommand\chapitre{Chapitre 2}
\newcommand\cours{PFE}
\usepackage{enumitem}
\everymath{\displaystyle}

\title{\titre }
\author{\auteur}
\date{\dateDoc}

\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}

\lhead{\cours}
\chead{}
\rhead{\currentname}
\lfoot{\titre}
\cfoot{}
\rfoot{Page \thepage\ /\ \pageref*{LastPage}}  


\lstset{
language=Matlab,
}

\hypersetup {
 pdftitle={\titre},    % title
    pdfauthor={\auteur},     % author
    pdfsubject={\cours},   % subject of the document
    pdfkeywords={}, % list of keywords
}


\renewcommand{\lstlistingname}{Code}% Listing -> Code
\renewcommand{\lstlistlistingname}{Liste des \lstlistingname s}% List of Listings -> List of codes



\begin{document}
\thispagestyle{empty}
\maketitle
\tableofcontents
\newpage
\section{Introduction}
Le problème de transport optimal fut étudié dans sa forme primitive par \emph{Gaspard Monge} dans son \emph{Mémoire sur la théorie des déblais et des remblais} de 1871 \cite{monge}. Il s'intéressait au déplacement de tas de matériaux d'un site d'extraction à un site de construction en minimisant le coût de transport (le coût de déplacement d'une unité est le produit de la charge déplacée par la distance). \\
La formulation moderne a été introduite par \emph{Leonid Kantorovitch}, prix Nobel d'économie en 1975 pour sa contribution aux allocations optimales.  
\newpage

\section{Notions générales}
Introduisons des notions essentielles qui nous amèneront à définir le problème de transport optimal et sa formulation en terme de géodésiques. 
\subsection{Problème de transport optimal}
\begin{definition}{Transport}
Soient deux mesures de probabilité $\nu$ et $\mu$ sur $\RR^n$ munie de la tribu borélienne $\mathcal{B}(\RR^n)$. Un transport est une application $T\ :\ \RR^n\rightarrow\RR^n$ qui envoie la mesure $\mu$ sur $\nu$. C'est à dire : 
\begin{align}
\forall B\in\mathcal{B}(\RR^n),\quad \mu(T^{-1}(B))=\nu(B)
\label{eq:trspmesures}
\end{align}
C'est une relation de conservation de masse. Nous notons $T_{\#}\mu=\nu$. 
\end{definition}
Introduisons petit à petit des hypothèses simplificatrices mais pas aberrantes. Supposons que les mesures aient une densité : $\mu = f_0 dx$ et $\nu = f_1 dx$. La relation \eqref{eq:trspmesures} devient alors : 
\begin{align*}
\forall B \in\mathcal{B}(\RR^n),\quad \int_B f_1(y)dy&=\int_{T^{-1}(B)} f_0(x) dx \\
&= \int_B \sum_{x\in T^{-1}(y)} \left(\frac{f_0(x)}{|det\ \nabla T(x)|}\right)dy\\
\end{align*}
C'est à dire $f_1(y) = \sum_{x\in T^{-1}(y)} \left(\frac{f_0(x)}{|det\ \nabla T(x)|}\right)$.\\
Supposons de plus que le transport est injectif et lisse, i.e. $x\in T^{-1}(y) \leftrightarrow y=T(x)$. Nous obtenons alors l'équation dite de Jacobienne : 
\begin{align}
f_1(T(x)) = \frac{f_0(x)}{|det\ \nabla T(x)|}
\label{eq:jacobienne}
\end{align}

Notons $\mathcal{T}(f_0,f_1)$ l'ensemble des applications qui vérifient \eqref{eq:jacobienne}.

\begin{definition}{Coût}
Le coût est une application 
$$
\fonction{C}{\RR^n\times\RR^n}{\RR^+}{(x,y)}{C(x,y)}
$$
qui représente intuitivement le coût d'affecter $x$ en $y$.
\end{definition}

Nous pouvons alors donner la définition qui donne le titre à ce projet. 
\begin{definition}{Problème de transport optimal}
Soient, $f_0$ et $f_1$ deux densités de probabilité et un coût $C$. Le problème de transport optimal est alors le suivant.\\
Trouver $T\in\mathcal{T}(f_0,f_1)$ qui réalise le 
\begin{align}
\min_{T\in\mathcal{T}(f_0,f_1)} \int C(x,T(x)) dx
\label{eq:trspOpt}
\end{align}
\end{definition}

\remarques{\begin{enumerate}
\item Un ensemble de coût est particulièrement intéressant, ce sont ceux qui s'écrivent sous la forme $C(x,y) = \|x-y\|^p$. Et plus précisément dans notre problème $p=2$. Nous verrons que le problème de transport optimal possède alors de jolies propriétés. 
\item La valeur de \eqref{eq:trspOpt} est alors appelée distance $L^2$ de Wasserstein entre $f_0$ et $f_1$, et est notée $d(f_0,f_1)$.
\end{enumerate}
}
\begin{theoreme}{}
Dans le cas du coût quadratique, il existe une unique application de transport optimal qui s'écrit comme le gradient d'une fonction convexe. 
\begin{align}
T(x) = \nabla \Psi(x)
\label{eq:opttrspGrdient}
\end{align}
\end{theoreme}
\begin{preuve}

\end{preuve}
A partir de l'équation de la jacobienne \eqref{eq:jacobienne}, $\Psi$ est solution de l'équation de \emph{Monge - Ampère} 
\begin{align}
f_1(\nabla\Psi(x)) = \frac{f_0(x)}{det\ H\Psi(x)}
\label{eq:mongemapere}
\end{align}


\subsection{Formulation de J.D. Benamou et Y. Brenier}
Pour le moment, le coût ne dépend que des densités initiales et finale, cependant il semble pertinent de chercher à connaître ce qu'il se passe pendant le transport, cela nous amènera à prendre en compte la notion d'obstacles. 
Dans leur article de 1999 \cite{benamoubrenier}, J.D. Benamou et Y. Brenier ont donné une autre formulation au problème de transport optimal en réintroduisant le temps comme variable. Ils obtiennent alors une interprétation très simple du problème de transport optimal en terme de mécanique des fluides. 

\begin{theoreme}{Benamou, Brenier}
Soient $f_0$ et $f_1$ deux densités de probabilité assez régulières. Alors, 
\begin{align}
\min_{T\in\mathcal{T}(f_0,f_1)} \int\|x-T(x)\|^2dx =\min_{(f,v)\in\mathcal{C}_v}\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dtdx
\end{align}
Avec,
\begin{align*}
f(t,x)\ :&\ \RR\times\RR^n\rightarrow \RR \quad \text{la densité}\\
v(t,x)\ :&\ \RR\times\RR^n\rightarrow \RR^n \quad \text{champ de vecteurs vitesses}
\end{align*}
et 
\begin{align}
C_v=\left\{(f,v) | \frac{\partial f}{\partial t} + \text{div}_x (fv) =0,\ f(0,\cdot) = f_0,\ f(1,\cdot)=f_1 \right\}
\label{eq:contraintes}
\end{align}
\end{theoreme}
\begin{preuve}
Nous redonnons la preuve donnée par Benamou et Brenier en complétant les points qui nous semble obscures.\\
La preuve repose sur l'utilisation des variables lagrangiennes $X(t,x)$ qui décrivent le comportement d'un fluide en "suivant" les particules. 
\begin{align*}
X(0,x)=x\quad\frac{\partial X}{\partial t}(t,x) = v(t,X(t,x))
\end{align*}
Ainsi, pour toute fonctions $\phi$ :
\begin{align}
\int \phi(t,x)f(t,x) dxdt &=\int \phi(t,X(t,x))f_0(x)dxdt \label{eq:coordlagrangienn}\\ 
\int \phi(t,x)f(t,x)v(t,x) dxdt &= \int \frac{\partial X}{\partial t}(t,x)\phi(t,X(t,x))f_0(x)dxdt \nonumber
\end{align}
la première égalité vient du changement de variable $x=X(t,x)$, or, $f(t,X(t,x))$ décrit la densité de la particule de fluide déplacée le long du champ de vitesse $v$ à l'instant t, c'est donc la densité initiale en $x$, $f_0(x)$. \\

Remarquons que \eqref{eq:coordlagrangienn} et la condition $f(0,\cdot)=f_0,\ f(1,\cdot) = f_1$ impliquent que $T(x)=X(1,x)$ est un transport valide. Puis remarquons que : 
$$
\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dxdt = \int_{\RR^n}\int_0^1 f_0(x)\|v(t,X(t,x))\|^2dxdt
$$
En prenant comme fonction test $\phi(t,x) = \|v(t,x)\|^2$. Nous obtenons, 
\begin{align*}
&= \int_{\RR^n}\int_0^1  f_0(x)\left\|\frac{\partial X}{\partial t} (t,x)\right\|^2dxdt \\
&= \int_{\RR^n} f_0(x)\int_0^1 \left\|\frac{\partial X}{\partial t} (t,x)\right\|^2dtdx\\
&\leq \int_{\RR^n} f_0(x)\left\|\int_0^1\frac{\partial X}{\partial t} (t,x) dt \right\|^2dx \\
&\leq \int_{\RR^n} f_0(x)\left\| X(1,x)-X(0,x) \right\|^2dx \\
&\leq \int_{\RR^n} f_0(x)\left\| X(1,x)-x \right\|^2dx \\
\end{align*}

Or, $X(1,x) = M(x) =\nabla\Psi(x)$ par la propriété .. 
$$
\leq \int_{\RR^n} f_0(x)\left\| \nabla\Psi(x)-x \right\|^2dx
$$
Nous avons ainsi montré que : 
$$
\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dxdt \leq \int_{\RR^n} f_0(x)\left\| \nabla\Psi(x)-x \right\|^2dx = d(f_0,f_1)^2
$$
Et avec le choix $x(t,x) = x+t(\nabla\Psi(x)-x)$ on obtient que pour toute fonction test $\phi$ nous avons, 
$$
\int \phi(t,x)f(t,x)v(t,x)dtdx =\int (\nabla\Psi(x) -x)\phi(t,x+t(\nabla\Psi(x)-x)f_0(x)dxdt
$$
ce qui achève la preuve. 
\end{preuve}









\section{Pbm de transport optimal}
Soient deux densités $f_0$ et $f_1$ de même masse, suffisamment régulière, sur un domaine $[0,1]^d$.\\
Un transport de $f_0$ sur $f_1$ est une application $T$ telle que :
$$
f_0(x)=f_1(T(x))|det(\partial T(x))|
$$
Le transport optimal est celui qui minimise le coût $\int c(x,T(x)dx$ parmi tous les transports de $f_0$ sur $f_1$. Dans notre problème : $C(x,y) =\|x-y\|^2$. 

\section{Formulation de Benamou et Brenier}
On montre que la géodésique entre $f_0$ et $f_1$ est : 
$$
f(x,t)=f_0((1-t)Id+tT(x))|det((1-t)Id+t\partial T(x))|
$$
et que ce chemin minimise le problème suivant : 
$$
\min_{(f,v)\in C_v} \int_{[0,1]^d}\int_0^1 f(x,t)\|v(x,t)\|^2dtdx
$$

Où : $C_v= \{(f,v)|\partial_t f+div_x(v)=0;v(0,.)=0,v(1,.)=0,f(.,0)=f_0,f(.,1)=f_1\}$

On pose $m = fv$ pour obtenir le problème de minimisation 
$$
\min_{(f,m)\in C}J(m,f)=\int_{[0,1]^d}\int_0^1 \theta (m,f)dtdx
$$
avec $\theta (m,f) = \frac{\|m\|^2}{f} si f>0 \quad 0 si (m,f)=(0,0)\quad \infty sinon$

Rq : on a ramené le pbm de transport optimal à celui de trouver la géodésique entre les deux densités. 
\section{Méthodes numériques}
On présente le cas 1D qui nous a accaparé. \\
Carré espace temps : $[0,1]^2$ que l'on discrétise : 
$$
G_c = \{(x_i=\frac{i}{N},t_j=\frac{j}{P})|0\leq i\leq N,\ 0\leq j\leq P\}
$$
On notre $E_c =(\RR^2)^{G_c}$ l'espace des variables centrées et $ V=(m_{ij},f_{ij}) \quad 0\leq i\leq N,\ 0\leq j\leq P$ les variables discrétisées. \\

Dans le but de capturer l'équation de continuité, on introduit une grille décentrée : 
$$
G_s^x = \{(x_i=\frac{i+1/2}{N},t_j=\frac{j}{P})|-1\leq i\leq N,\ 0\leq j\leq P\}
$$
et 
$$
G_s^t = \{(x_i=\frac{i}{N},t_j=\frac{j+1/2}{P})|0\leq i\leq N,\ -1\leq j\leq P\}
$$

On note $E_s=\RR^{G_s^x}\times\RR^{G_s^t}$ l'espace des variables décentrées, et $U=(\bar{m_{ij}}\quad 1\leq i\leq N,\ 0\leq j\leq P,\bar{f_{ij}},\quad 0\leq i\leq N,\ -1\leq j\leq P$


\section{Opérateurs}
On introduit plusieurs opérateurs pour lier les variables décentrées et les variables centrées.
\subsection{Interpolation}
$$
I:E_s \rightarrow E_c
$$
tq 
$$
m_{ij} = = (\bar{m}_{i+1/2,j}+\bar{m}_{i-1/2,j})/2.
$$
et
$$
f_{ij} = = (\bar{f}_{i,j+1/2}+\bar{f}_{i,j-1/2})/2.
$$
Cet opérateur s'interprète matriciellement : 
$$
m = \bar{m}I_m
$$
et 
$$
f = I_f\bar{f}
$$

\subsection{Divergence}
l'opérateur qui approxime la divergence
$$
div : E_s\rightarrow \RR^{G_c}
$$
et 
$$
div(\bar{m},\bar{f})_{ij} = (\bar{m}_{i+1/2,j}-\bar{m}_{i-1/2,j}) + (\bar{f}_{i,j+1/2}-\bar{f}_{i,j-1/2})
$$

\subsection{Frontières}
Un opérateur pour extraire les frontières : 

$$
b(\bar{m},\bar{f}) = ((\bar{m}_{-1/2,j},\bar{m}_{N+1/2,j});(\bar{f}_{i,-1/2},\bar{f}_{i,P+1/2}))
$$
et on impose les conditions aux frontières suivantes : 
$$
b(\bar{m},\bar{f}) = b_0=(0,0,f_0,f_1);
$$

\subsection{Problème discrétisé}

On a le problème suivant 

$$
\min_{U=(\bar{m},\bar{f})\in E_s} \theta(I(\bar{m},\bar{f})) + \iota_C(U)
$$
avec l'ensemble des contraintes :
$$
C=\{(\bar{m},\bar{f})\in E_s|\ div(\bar{m},\bar{f}) = 0\ b(\bar{m},\bar{f}) = b_0 \}
$$

\section{Résolution par algorithme de séparation de proximité}
Ces algorithmes sont des généralisation des algos de gradient conjugués. \\

\textbf{Remarque : } On a $J(m,f)=\int_{[0,1]^d}\int_0^1 \frac{\|m\|^2}{f} dtdx$ donc si $f\rightarrow\infty$ on a $J\rightarrow 0$ donc ce n'est pas coercif ce qui rend l'existence de minimiseurs non triviale. Et si $f\rightarrow 0$, on a $j\rightarrow \infty$ donc les méthodes de gradient conjugués ne peuvent pas s'appliquer, le gradient n'est pas lipschitz. \\

On veut résoudre le pbm suivant : 
$$
\min_{z=(U,V)\in E_s\times E_c} G_1(z)+G_2(z)
$$
où, $G_1(z) = J(U)+\iota_C(U)$ et $G_2(z)=\iota_{C_s}(z)$ et $C_s=\{z=(U,V)\in E_s\times E_c\ | \ V=I(U) \}$


\remarque{$G_1$ est la fonctionnelle originelle et $G_2$ vient de notre introduction des variables décalées. }\\

On va alors calculer les opérateurs de proximités de $G_1$ et $G_2$.On dit que $G_1$ est \textbf{simple} dans la mesure ou : 

$$
Prox_{\gamma G_1} (U,V) = (Prox_{\gamma C} (U),Prox_{\gamma J} (V))
$$

\subsection{Opérateur de proximité de $G_2$}

$$
Prox_{\gamma C_s} (U,V) = arg\min_{z'\in C_s} \frac{1}{2}\|z-z'\|^2= Proj_{C_s}(U,V)
$$
Suivant l'article de Papadakis, Peyré et Oudet on a :

$$
Prox_{\gamma C_s} (U,V)  = (\tilde{U},I(\tilde{U}))
$$
et 

$$
\tilde{U} = (Id +I^{\star}I)^{-1}(U+I^{\star}(V))
$$
Ou $I^{\star}$ est l'adjointe de $I$. ce qui nous donne en terme de matrice : 
$$
\tilde{\bar{m}} = (\bar{m}+mI_m^{\star})(Id +I_mI_m^{\star})^{-1}
$$
et 
$$
\tilde{\bar{f}} = (Id +I_f^{\star}I_f)^{-1}(\bar{f}+I_f^{\star}f)
$$
enfin : $\tilde{m}=\tilde{\bar{m}}I_m$ et $\tilde{f}=I_f\tilde{\bar{f}}$

Comme l'opérateur d'interpolation s'interprète matriciellement, son adjoint est donné par la transposée. 

\subsection{Opérateur de proximité de $J$}

$Prox_{\gamma J}(V)=(Prox_{\gamma J}(V_k))_{k\in G_c}$ et 

$$
Prox_{\gamma J}(m_k,f_k) = (\frac{f^{\star}_km_k}{f^{\star}_k+2\gamma},f^{\star}_k)\ si\ f^{\star}>0 \qquad (0,0)\ sinon
$$ 
et $f^{\star}_k$ est la plus grande racine réelle du polynôme de degré 3 donné par : 
$$
P(x) = (X-f_k)(X+2\gamma)^2-\gamma \|m_k\|^2
$$
La racine est calculée rapidement avec l’algorithme de Newton. 

\subsection{Opérateur de proximité de $\iota_C$}

$$
Prox_{\gamma C} = Proj_C
$$
on réécrit l'ensemble des contraintes sous la forme : 

$$
C=\{U\in E_s\ | \ AU=y\} \qquad A=(div,b)^T \qquad y = (0,b0)^T
$$

et on obtient $Proj_C(U)=(Id +A^{\star}\Delta^{-1}A)U + A^{\star} \Delta^{-1}y$ avec $\Delta^{-1}=(A^{\star}A)^{-1}$ et son problème requiert la résolution d'un problème de poisson. 

\section{Algorithme de Douglas Rachford}
On procède à l'itération suivante : $(z^l,w^l)\in (E_s\times E_c)\times (E_s\times E_c)$ avec un $w^0$ donné : 
$$
z^{l+1}=Prox_{\gamma G_2} (w^l)
$$
et 
$$
w^{l+1}=(1-\frac{\alpha}{2})w^l + \frac{\alpha}{2}RProx_{\gamma G_2}\circ RProx_{\gamma G_1} (w^l)
$$

Où on a posé : $RProx_{\gamma G} = 2Prox_{\gamma G}-Id$.\\
On montre alors que pour $\alpha\in ]0,2[$ et $\gamma>0$ la méthode converge $z^l\rightarrow z^{\star}$ ce qui permet de retrouver la géodésique, car : 
$$
z^l=(U^l,V^l)\rightarrow (U^{\star},V^{\star}) \qquad V^{\star}=(m^{\star},f^{\star})
$$


\bibliographystyle{plain}
\bibliography{biblio.bib}
\appendix


\newpage
\addcontentsline{toc}{section}{\listfigurename}
\listoffigures
\addcontentsline{toc}{section}{\lstlistlistingname}
\lstlistoflistings


\end{document}
