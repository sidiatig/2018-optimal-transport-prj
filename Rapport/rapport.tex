\documentclass[a4paper,12pt]{article}

\input{modele_documents.tex}

\setlength{\parindent}{0pt}
\newcommand\titre{OT}
\newcommand\auteur{Timothée \textsc{Schmoderer}}
\newcommand\dateDoc{2017/2018}
\newcommand\chapitre{Chapitre 2}
\newcommand\cours{PFE}
\usepackage{enumitem}
\everymath{\displaystyle}

\title{\titre }
\author{\auteur}
\date{\dateDoc}

\usepackage{fancyhdr,lastpage}
\pagestyle{fancy}

\lhead{\cours}
\chead{}
\rhead{\currentname}
\lfoot{\titre}
\cfoot{}
\rfoot{Page \thepage\ /\ \pageref*{LastPage}}  


\lstset{
language=Matlab,
}

\hypersetup {
 pdftitle={\titre},    % title
    pdfauthor={\auteur},     % author
    pdfsubject={\cours},   % subject of the document
    pdfkeywords={}, % list of keywords
}


\renewcommand{\lstlistingname}{Code}% Listing -> Code
\renewcommand{\lstlistlistingname}{Liste des \lstlistingname s}% List of Listings -> List of codes



\begin{document}
\thispagestyle{empty}
\maketitle
\tableofcontents
\newpage
\section{Introduction}
Le problème de transport optimal fut étudié dans sa forme primitive par \emph{Gaspard Monge} dans son \emph{Mémoire sur la théorie des déblais et des remblais} de 1781 \cite{monge}. Il s'intéressait au déplacement de tas de matériaux d'un site d'extraction à un site de construction en minimisant le coût de transport (le coût de déplacement d'une unité est le produit de la charge déplacée par la distance). \\
La formulation moderne a été introduite par \emph{Leonid Kantorovitch}, prix Nobel d'économie en 1975 pour sa contribution aux allocations optimales.  \\


Nous essaierons d'être le plus exhaustif possible dans la présentation des résultats, la plupart des énoncés seront démontrés. 



Le plan de rapport est le suivant, nous commencerons par présenté les notions liées au transport optimal, puis nous ferrons un court aparté sur les opérateurs proximaux. Enfin, Nous attaquerons la discrétisation et la mise en œuvre numérique. 
{\Huge Plan : \\
1 - Théorie sur le transport optimal \\
2 - Théorie sur les opérateurs proximaux  \\
3 - Numérique \\
	3.1 - Idées générales de l'implémentation \\
	3.2 - Implémentation Grosse matrice \\
	3.3 - Implemetation centrée \\
		3.3.1 - 1D \\
		3.3.2 - 2D \\
	3.4 - Implementation staggered FFT \\
4 - Galerie du Fun \\

}
\newpage

\section{Notions générales}
Introduisons des notions essentielles qui nous amèneront à définir le problème de transport optimal et sa formulation en terme de géodésiques. 
\subsection{Problème de transport optimal}
\begin{definition}{Transport}
Soient deux mesures de probabilité $\nu$ et $\mu$ sur $\RR^n$ munie de la tribu borélienne $\mathcal{B}(\RR^n)$. Un transport est une application $T\ :\ \RR^n\rightarrow\RR^n$ qui envoie la mesure $\mu$ sur $\nu$. C'est à dire : 
\begin{align}
\forall B\in\mathcal{B}(\RR^n),\quad \mu(T^{-1}(B))=\nu(B)
\label{eq:trspmesures}
\end{align}
C'est une relation de conservation de masse. Nous notons $T_{\#}\mu=\nu$. 
\end{definition}
Introduisons petit à petit des hypothèses simplificatrices mais pas aberrantes. Supposons que les mesures aient une densité : $\mu = f_0 dx$ et $\nu = f_1 dx$. La relation \eqref{eq:trspmesures} devient alors : 
\begin{align*}
\forall B \in\mathcal{B}(\RR^n),\quad \int_B f_1(y)dy&=\int_{T^{-1}(B)} f_0(x) dx \\
&= \int_B \sum_{x\in T^{-1}(y)} \left(\frac{f_0(x)}{|det\ \nabla T(x)|}\right)dy\\
\end{align*}
C'est à dire $f_1(y) = \sum_{x\in T^{-1}(y)} \left(\frac{f_0(x)}{|det\ \nabla T(x)|}\right)$.\\
Supposons de plus que le transport est injectif et lisse, i.e. $x\in T^{-1}(y) \leftrightarrow y=T(x)$. Nous obtenons alors l'équation dite de Jacobienne : 
\begin{align}
f_1(T(x)) = \frac{f_0(x)}{|det\ \nabla T(x)|}
\label{eq:jacobienne}
\end{align}

Notons $\mathcal{T}(f_0,f_1)$ l'ensemble des applications qui vérifient \eqref{eq:jacobienne}.

\begin{definition}{Coût}
Le coût est une application 
$$
\fonction{C}{\RR^n\times\RR^n}{\RR^+}{(x,y)}{C(x,y)}
$$
qui représente intuitivement le coût d'affecter $x$ en $y$.
\end{definition}

Nous pouvons alors donner la définition qui donne le titre à ce projet. 
\begin{definition}{Problème de transport optimal}
Soient, $f_0$ et $f_1$ deux densités de probabilité et un coût $C$. Le problème de transport optimal est alors le suivant.\\
Trouver $T\in\mathcal{T}(f_0,f_1)$ qui réalise le 
\begin{align}
\min_{T\in\mathcal{T}(f_0,f_1)} \int C(x,T(x)) f_0(x)dx
\label{eq:trspOpt}
\end{align}
\end{definition}

\remarques{\begin{enumerate}
\item Un ensemble de coût est particulièrement intéressant, ce sont ceux qui s'écrivent sous la forme $C(x,y) = \|x-y\|^p$. Et plus précisément dans notre problème $p=2$. Nous verrons que le problème de transport optimal possède alors de jolies propriétés. 
\item La valeur de \eqref{eq:trspOpt} est alors appelée distance $L^2$ de Wasserstein entre $f_0$ et $f_1$, et est notée $d(f_0,f_1)$.
\end{enumerate}
}

\begin{theoreme}{Existence et unicité du transport optimal}

\end{theoreme}
\begin{preuve}

\end{preuve}

\begin{theoreme}{}
Dans le cas du coût quadratique, il existe une unique application de transport optimal qui s'écrit comme le gradient d'une fonction convexe. 
\begin{align}
T(x) = \nabla \Psi(x)
\label{eq:opttrspGrdient}
\end{align}
\end{theoreme}
\begin{preuve}

\end{preuve}
A partir de l'équation de la jacobienne \eqref{eq:jacobienne}, $\Psi$ est solution de l'équation de \emph{Monge - Ampère} 
\begin{align}
f_1(\nabla\Psi(x)) = \frac{f_0(x)}{det\ H\Psi(x)}
\label{eq:mongemapere}
\end{align}


\subsection{Formulation de J.D. Benamou et Y. Brenier}
Pour le moment, le coût ne dépend que des densités initiales et finale, cependant il semble pertinent de chercher à connaître ce qu'il se passe pendant le transport, cela nous amènera à prendre en compte la notion d'obstacles. 
Dans leur article de 1999 \cite{benamoubrenier}, J.D. Benamou et Y. Brenier ont donné une autre formulation au problème de transport optimal en réintroduisant le temps comme variable. Ils obtiennent alors une interprétation très simple du problème de transport optimal en terme de mécanique des fluides. 

\begin{theoreme}{Benamou, Brenier}
Soient $f_0$ et $f_1$ deux densités de probabilité assez régulières. Alors, 
\begin{align}
\min_{T\in\mathcal{T}(f_0,f_1)} \int\|x-T(x)\|^2dx =\min_{(f,v)\in\mathcal{C}_v}\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dtdx
\end{align}
Avec,
\begin{align*}
f(t,x)\ :&\ \RR\times\RR^n\rightarrow \RR \quad \text{la densité}\\
v(t,x)\ :&\ \RR\times\RR^n\rightarrow \RR^n \quad \text{champ de vecteurs vitesses}
\end{align*}
et 
\begin{align}
C_v=\left\{(f,v)\ |\ \frac{\partial f}{\partial t} + \text{div}_x (fv) =0,\ f(0,\cdot) = f_0,\ f(1,\cdot)=f_1 \right\}
\label{eq:contraintes}
\end{align}
\end{theoreme}
\begin{preuve}
Nous redonnons la preuve donnée par Benamou et Brenier en complétant les points qui nous semble obscurs.\\


\begin{minipage}{0.5\linewidth}
La preuve repose sur l'utilisation des variables lagrangiennes $X(t,x)$ qui décrivent le comportement d'un fluide en "suivant" les particules. 
Supposons $f_0$ et $f_1$ bornées et à support compact de $\RR^n$. Considérons $(f,v)$, suffisamment régulières, vérifiant \eqref{eq:contraintes}.
\end{minipage}\hfill
\begin{minipage}{0.55\linewidth}
\begin{center}
\includegraphics[scale=0.3]{img/lagrange.png}
\end{center}
\end{minipage}


En termes formels, cela donne, 
\begin{align}
X(0,x)=x\quad\frac{\partial X}{\partial t}(t,x) = v(t,X(t,x))
\label{eq:defcoordlag}
\end{align}
Ainsi, pour toute fonction test $\phi$ :
\begin{align}
\int \phi(t,x)f(t,x) dxdt &=\int \phi(t,X(t,x))f_0(x)dxdt \label{eq:coordlagrangienn}\\ 
\int \phi(t,x)f(t,x)v(t,x) dxdt &= \int \frac{\partial X}{\partial t}(t,x)\phi(t,X(t,x))f_0(x)dxdt 
\end{align}
la première égalité vient du changement de variable $x=X(t,x)$, or, $f(t,X(t,x))$ décrit la densité de la particule de fluide déplacée le long du champ de vitesse $v$ à l'instant t, c'est donc la densité initiale en $x$, $f_0(x)$. \\

Remarquons que \eqref{eq:coordlagrangienn} et la condition $f(0,\cdot)=f_0,\ f(1,\cdot) = f_1$ impliquent que $T(x)=X(1,x)$ est un transport valide. En effet, pour tout borélien $B$, $\int_{T(x)\in B}f_0(x) dx = \int_{X(1,x)\in B}f_0(x) dx$ Ce qui signifie, qu'au temps $1$, la particule $x$ est dans le borélien $B$, comme la densité au temps $1$ est donnée par $f_1$ on a bien : $\int_{X(1,x)\in B}f_0(x) dx = \int_B f_1(x)dx$.


Puis remarquons, en prenant comme fonction test $\phi(t,x) = \|v(t,x)\|^2$ dans \eqref{eq:coordlagrangienn}, que: 
$$
\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dxdt = \int_{\RR^n}\int_0^1 f_0(x)\|v(t,X(t,x))\|^2dxdt
$$
Nous obtenons, 
\begin{align*}
&= \int_{\RR^n}\int_0^1  f_0(x)\left\|\frac{\partial X}{\partial t} (t,x)\right\|^2dxdt \quad \text{par } \eqref{eq:defcoordlag}\\
&= \int_{\RR^n} f_0(x)\int_0^1 \left\|\frac{\partial X}{\partial t} (t,x)\right\|^2dtdx\\
&\text{par l'inégalité de Jensen : }\\
&\leq \int_{\RR^n} f_0(x)\left\|\int_0^1\frac{\partial X}{\partial t} (t,x) dt \right\|^2dx \\
&\leq \int_{\RR^n} f_0(x)\left\| X(1,x)-X(0,x) \right\|^2dx \\
&\leq \int_{\RR^n} f_0(x)\left\| X(1,x)-x \right\|^2dx  \quad \text{par } \eqref{eq:defcoordlag}\\
\end{align*}

Nous reconnaissons ici la forme qui apparaît dans la distance $L^2$ de Wasserstein, pour le transport $T(x) = X(1,x)$. Prenons $\nabla\Psi(x)$ le transport optimal, nous avons alors :  
$$
\leq \int_{\RR^n} f_0(x)\left\| \nabla\Psi(x)-x \right\|^2dx
$$
Finalement, nous avons montré que : 
$$
\int_{\RR^n}\int_0^1 f(t,x)\|v(t,x)\|^2dxdt \leq \int_{\RR^n} f_0(x)\left\| \nabla\Psi(x)-x \right\|^2dx = d(f_0,f_1)^2
$$
Il reste un trouver un couple $(f,v)$ tel que nous ayons l'égalité et la preuve sera complète. Choisissons $X(t,x) = x+t(\nabla\Psi(x)-x)$, ce qui correspond à la paire (f,v) vérifiant pour toute fonction test $\phi$ : 
\begin{align*}
\int_{\RR^n} \int_0^1 \phi(t,x)f(t,x)dtdx &=\int_{\RR^n} \int_0^1 \phi(t, x+t(\nabla\Psi(x)-x)) f_0(x)dxdt \\
\int_{\RR^n} \int_0^1 \phi (t,x)f(t,x)v(t,x) dtdx &= \int_{\RR^n} \int_0^1 \left(\nabla\Psi (x) - x\right) \phi(t, x+t(\nabla\Psi(x)-x)) f_0(x)  dtdx
\end{align*}


Et en reprenant le raisonnement de la première étape, nous obtenons
\begin{align*}
\int_{\RR^n} \int_0^1 f(t,x)\|v(t,x)\|^2dtdx &= \int_{\RR^n} \int_0^1 f_0(x)\|v(t,X(t,x)\|^2\\
&= \int_{\RR^n} \int_0^1 f_0(x) \|\frac{\partial X(t,x)}{\partial t}(t,x)\|^2 dtdx \\
&= \int_{\RR^n} f_0(x) \|\nabla \Psi(x) -x\|^2dx \\
\end{align*}
Ce qui achève la preuve. 
\end{preuve}

La première équation dans l'ensemble des contraintes correspond à une équation de continuité. Tandis que les conditions sur les bords temporels imposent l'attache au problème initial. 

\subsection{Une autre utilité au transport optimal}
Dans les années 1990, la théorie du transport optimal et devenu un outil de démonstration puissant et élégant pour démontere de nombreuses inégalités géométriques et fonctionnelles. \\
L'exemple le plus simple est fourni par \emph{l'inégalité isopérimétrique}, solution du problème de Didon \cite{Didon}. 
Ce problème, peut s'"noncer ainsi : de toutes les courbes fermées de longueur donnée, celle qui entoure l'aire al plus grande est le cercle. Ce problème ce généralise, en diemnsion $n$.
\begin{theoreme}{Problème de Didon}
Dans l'espace euclidien $\RR^n$ muni de la mesure de Lebesgue $\lambda_n$, pour tout compact $K$, 
$$
\frac{(\lambda_{n-1} B)^n}{(\lambda_n B) ^n} \leq \frac{(\lambda_{n-1} K)^n}{(\lambda_n K) ^n}
$$
Avec $B$, la boule unité. 
\end{theoreme}


En particulier en dimension $n=3$ 
\begin{preuve}
Considérons les fonctions de densité uniforme et d'untégrale 1, du'ne part sur le domaine de volume $V$ et d'autre part sur une boule unité, donc de volume $\frac{4\pi}{3}$. Cela induit l'éxistence d'un potentiel $\phi$ permettant le transport de la première vers la second, solution de l'équation de Monge - Ampère en tout point $x$ du volume $V$ : 
$$
\frac{1}{V}=det(I + H\phi (x))\frac{1}{B}
$$

\end{preuve}


\subsection{Conclusion}
Dans cette partie, nous avons introduit le problème de transport optimal. Démontrer l'existence et l'unicité du transport. 
Le problèem se reformule alors. 

Et nous avons montrer avec Benamou et Brenier que ce problème est équivalent au suivant : 

qui a une interprétation en terme de mécanique des fluides très pratiques. \\

Avant d'enatmmer la résolution numérique du problème de transport optimal, nous présentons les opérateurs proximaux qui joueront un rôle central dans notre modèle. 

\newpage
\section{Opérateurs proximaux}
L'opérateur proximal d'une fonction convexe généralise la notion de projection sur un ensemble convexe. Cette notion fût introduite pour répondre aux problèmes de minimisation de fonctions non nécessairement différentiables. 

Suivant la présentation de \emph{P.L. Combettes \emph{et} JC. Pesquet} \cite{combettes} nous présentons une série de résultats sur les opérateurs proximaux, en complétant quelques preuves.
\paragraph{Notation :}Le domaine d'une fonction $f:\ \RR^n\longrightarrow\ ]-\infty,+\infty]$ est $\mathcal{D}(f) = \{ x\in\RR^n\ |\ f(x) < +\infty \} $. Notons $\Gamma_0(\RR^n)$ l'ensemble des fonctions convexes semi-continues inférieurement à valeurs dans $]-\infty,+\infty]$ telles que $\mathcal{D}(f) \neq \emptyset$. 

\begin{definition}{Sous - différentiel}
Soit $f\in\Gamma_0(\RR^n)$, le sous - différentiel de $f$ est l'application suivante,
$$
\fonction{\partial f}{\RR^n}{\mathcal{P}(\RR^n)}{x}{\left\{ u\in\RR^n\ |\ \forall y\in\RR^n,\ \langle u,y-x\rangle +f(x)\leq f(y) \right\}}
$$ 
Avec $\mathcal{P}(\RR^n)$, l'ensemble des parties de $\RR^n$.
\end{definition}


L'interprétation géométrique du sous-différentiel est la suivante. Il est formé par toutes les directions des hyperplan qui passent par le point $(x,f(x))$ et restent "sous" le graphe de la fonction $f$. \\

\exemple{Calculons le sous différentiel en tout point de 
$$
\fonction{f}{\RR}{\RR^+}{x}{|x|}
$$
Considérons deux cas, 
\begin{enumerate}
\item $\mathrm{x>0}$, le cas $x<0$ est identique. Soit une direction $u$ appartient au sous différentiel de $f$ en x, alors $\forall y\in \RR$ : 
\begin{align*}
u(y-x) + f(x) \leq f(y)\\
u(y-x) + x \leq f(y)\\
\end{align*}
En particulier, pour $y = 0$ on obtient $x(1-u) \leq 0$, ainsi $u\geq 1$. Et donc en prenant $y > x$ (en particulier, $y>0$) on obtient $u(y-x) + x \geq y$ ce qui est contraire à notre définition sauf si $u=1$. 
\item $\mathrm{x=0}$ Soit $u\in \partial f(0)$, alors $u$ doit vérifier pour tout $y\in \RR$,
\begin{align*}
uy\leq |y|
\end{align*}
En prenant $y>0$ on obtient que $u\leq 1$ et en prenant $y<0$, on a $u\geq -1$.
\end{enumerate}
En somme, 
$$
\partial f(x) =
\left\{
\begin{array}{cc}
\{-1\} & x <0 \\
\left[-1,1\right] & x =0\\
\{1\} & x>0\\
\end{array}
\right.
$$
Ce qui correspond bien à l'interprétation géométrique donnée et à la figure \ref{fig:subgradient}.
\begin{figure}[!h]
\centering
\includegraphics[scale=0.7]{img/sub_gradient.png}
\caption{\label{fig:subgradient}Illustration du sous - différentiel de $|x|$ en $0$}
\end{figure}
}

\begin{propriete}
Soit $f\in\Gamma_0(\RR^n)$. Si $f$ est différentiable sur son domaine, alors 
$$
\forall x\in \mathcal{D}(f)^{\circ},\ \partial f(x) = \{\nabla f(x)\}
$$
\end{propriete}
\begin{preuve}
Comme une $f$ est convexe différentiable, nous avons la caractérisation suivante du gradient : 
$$
\forall x,y\in\RR^n,\quad \langle \nabla f (x) ,y-x\rangle +f(x) \leq f(y)
$$
qui implique que $\nabla f(x) \in\partial f(x)$.\\
Supposons à présent que $p\in\partial f(x)$. Soit une suite $y_n^1$ de $\RR^n$ qui tend vers x. Écrivons, $y_n^1=x+k_n$ avec $\|k_n\|\rightarrow 0$. Posons alors $y_n^2$ une seconde suite de $\RR^n$ telle que $y_n^2= x-k_n$, alors : 
$$
\left\{
\begin{array}{cc}
\langle p, k_n\rangle &\leq f(x +k_n) - f(x) \\
\langle p,-k_n\rangle & \leq f(x-k_n)-f(x) \\
\end{array}
\right.
$$
Comme $f$ est différentiable dans son domaine, on peut écrire, 
$$
f(x\pm k_n)=f(x) \pm \langle \nabla f(x), k_n\rangle
$$
Ainsi, nous obtenons
$$
\left\{
\begin{array}{cc}
\langle p, k_n\rangle &\leq \langle \nabla f(x), k_n\rangle \\
\langle p, k_n\rangle &\geq \langle \nabla f(x), k_n\rangle \\
\end{array}
\right.
$$
Ainsi $\langle p,k_n \rangle = \langle \nabla f(x),k_n\rangle$. En faisant tendre $n$ vers $+\infty$, comme la suite $k_n$ converge fortement vers $0$, elle va aussi converger faiblement. Ce qui implique que $p=\nabla f(x)$. 
\end{preuve}
Cette propriété est cohérente avec l'exemple précédent, $\partial f(x) =f'(x)$ partout où, $f$ est différentiable, c'est à dire $x\neq 0$. 

\begin{propriete}
Si $f = \alpha f_1+\beta f_2$, avec $f_1,\ f_2\in\Gamma_0(\RR^n)$ et $\alpha,\ \beta \geq 0$. Alors 
\begin{align*}
\partial f(x) &= \alpha \partial f_1(x) + \beta \partial f_2(x) \\
&= \left\{u\in\RR^n\ |\ \exists (u_1,u_2)\in \partial f_1(x)\times f_2(x),\ u = \alpha u_1 + \beta u_2 \right\}
\end{align*}
\end{propriete}
\begin{preuve}
\vspace{-1cm}
\paragraph{$\alpha\partial f_1(x) +\beta \partial f_2(x) \subset
\partial (\alpha f_1 +\beta f_2)(x)$ :}$u\in\RR^n$ est dans $\partial (\alpha f_1 +\beta f_2)(x)$ si et seulement si pour tout $y\in \RR^n$, nous avons 
$$
\langle u,y-x\rangle + \alpha f_1(x) +\beta f_2(x) \leq  \alpha f_1(y) +\beta f_2(y) 
$$
En particulier en prenant $u\in \alpha\partial f_1(x) +\beta \partial f_2(x) $, au sens donné dans la propriété, cette inégalité est vérifiée. 

\paragraph{$\alpha\partial f_1(x) +\beta \partial f_2(x) \supset
\partial (\alpha f_1 +\beta f_2)(x)$ :}Soit $u\in\partial (\alpha f_1 +\beta f_2)(x)$, par contradiction, supposons que  $u\ni \alpha\partial f_1(x) +\beta \partial f_2(x)$, c'est à dire : 
$$
\forall (u_1,u_2)\in \partial f_1(x) \times \partial f_2(x),\quad u\neq \alpha u_1+\beta u_2
$$
{\Huge EMPTY}
\end{preuve}


La notion de sous - différentiel est centrale car il permet de caractériser les minimiseurs d'une fonctions (même non différentiable).
\begin{theoreme}{}
Soit $f\in\Gamma_0(\RR^n)$. Le point $x^{\star}\in\RR^n$ est un point de minimum global de $f$ si et seulement si 
$$
0\in \partial f(x^{\star})
$$ 
\end{theoreme}
\begin{preuve}
\vspace{-1cm}
\paragraph{$\Rightarrow$} Soit $x^{\star}$ le minimum global de $f$ alors $\forall y\in \RR^n$ :
\begin{align*}
f(x^{\star}) &\leq f(y)\\
\langle 0,y-x\rangle +f(x^{\star}) &\leq f(y)\\
\end{align*}
Et donc $0\in \partial f(x^{\star})$. 
\paragraph{$\Leftarrow$} Si $0\in \partial f(x^{\star})$ alors $\forall y\in \RR^n$ :
\begin{align*}
\langle 0,y-x^{\star}\rangle + f(x^{\star}) &\leq f(y)\\
f(x^{\star}) \leq f(y) 
\end{align*}
et $x^{\star}$ est un minimum global de $f$. 
\end{preuve}

L'opérateur proximal est un outil important dans l'optimisation de fonctions convexes non différentiables. C'est une application minimisant une fonction fortement convexe dépendant de la fonction originale $f$. En trouvant un compormis entre trouver le minimum de la fonction non différentaible $f$ et en forçant une proximité avec l'argument, nous approchons le minimum de $f$. 

\begin{definition}{Opérateur proximal}
Soit $f\in\Gamma_0(\RR^n)$. L'opérateur proximal de $f$ est noté $prox_f$ et il est défini pour tout $x\in\RR^n$ par : 
$$
prox_f(x) = \argmin_{y\in\RR^n} f(y) +\frac{1}{2}\|x-y\|^2
$$
\end{definition}
\exemple{Prenons, l'indicatrice d'un ensemble convexe non vide $C$ : 
$$
\mathrm{i}_C = \left\{
\begin{array}{cc}
0 & x\in C\\
+\infty & x \notin C
\end{array}
\right.
$$
Alors, 
\begin{align*}
prox_{\mathrm{i}_C}(x) &= \arg \min_{y\in\RR^n} \mathrm{i}_C(y) + \frac{1}{2}\|x-y\|^2\\
&= \arg\min_{y\in C} \frac{1}{2}\|x-y\|^2\\
&= \mathrm{P}_C (x)\\ 
\end{align*}
Où, $\mathrm{P}_C$ désigne la projection orthogonale sur $C$. 
}\\
Sur la figure \ref{fig:proximal} (tirée de \cite{parikh2014proximal}), nous avons en \textbf{gras} la frontière de la fonction $f$, les lignes fines sont les lignes de niveaux de $f$. Évaluer $prox_f$ sur les points bleus les associent aux points rouges correspondant. Les trois points dans le domaine restent dans le domaine et se déplacent vers le minimum, et les deux à l'extérieur du domaine se placent que la frontière de celui-ci en direction du minimum. 
\begin{figure}[!h]
\centering
\includegraphics[scale=0.7]{img/proximal.png}
\caption{\label{fig:proximal}Illustration de l'opérateur proximal}
\end{figure}
Un scalaire $\gamma >0 $ peut être introduit dans cette définition $prox_{\gamma f}$, ce paramètre contrôle le poids apporté à la minimisation de $f$. Un $\gamma$ grand, forcera de se rapprocher fortement du minimum de $f$, alors qu'un $\gamma$ petit favorisera les points proches de $x$. Le résultat important est le suivant. 
\begin{propriete}
Soit une fonction $f$ convexe semi continue inférieurement. Alors, $prox_f(x)$ existe et est unique pour tout $x\in\RR^n$. De plus, il est caractérisé par : 
$$
\forall (x,p)\in \RR^n\times\RR^n\quad p=prox_fx\quad\Longleftrightarrow\quad x-p\in\partial f(p)
$$
En particulier, si $f$ est différentiable alors : 
$$
\forall (x,p)\in \RR^n\times\RR^n\quad p=prox_fx\quad \Longleftrightarrow \quad x-p=\nabla f(p)
$$
\end{propriete}
\begin{preuve}
$\forall x \in \RR^n$, la fonction $g\ :\ y \rightarrow f(y) +\frac{1}{2}\|x-y\|^2$ est strictement convexe et dans $\Gamma_0(\RR^n)$ (i.e. identiquement égale à $+\infty$). Ce qui montre que le minimum de $g$ est toujours atteint. 


\paragraph{$\Leftarrow$} Supposons que $x-p\in\partial f(p)$. Alors, pour tout $y\in\RR^n$, nous avons 
$$
\langle x-p,y-p\rangle +f(p) \leq f(y)
$$
Or,
\begin{align*}
\langle x-p,y-p\rangle &= \frac{1}{2}\langle x-p,y-x+x-p\rangle + \frac{1}{2}\langle  x-y+y-p,y-p\rangle\\
&=\frac{1}{2} \|x-p\|^2 + \frac{1}{2}\|y-p \|^2 +\frac{1}{2}\left( \langle x-p,y-x\rangle + \langle x-y,y-p\rangle\right)\\
&= \frac{1}{2} \|x-p\|^2 + \frac{1}{2}\|y-p \|^2 -\frac{1}{2}\|x-y\|^2
\end{align*}
Ainsi,
\begin{align*}
\frac{1}{2} \|x-p\|^2 + \frac{1}{2}\|y-p \|^2 -\frac{1}{2}\|x-y\|^2 + f(p) &\leq f(y) \\
\frac{1}{2} \|x-p\|^2 + f(p) &\leq f(y) + \frac{1}{2}\|x-y\|^2\\
\end{align*}
Donc $p=prox_f(x)$.
\paragraph{$\Rightarrow$} Soit $(x,p)\in \RR^n\times\RR^n$ telle que $p=prox_f(x)$. Alors $\forall y\in\RR^n$,
{\Huge je me demande si c'est pas eja prouvé}
\end{preuve}
 
De plus, 
\begin{propriete}
Le point $x^{\star}$ minimise $f$ si et seulement si 
$$
x^{\star} = prox_f(x^{\star})
$$
\end{propriete} 
\begin{preuve}
\vspace{-1cm}
\paragraph{$\Rightarrow$} Supposons que $x^{\star}$ minimise $f$ alors, comme $\|x^{\star}-y\|^2 > 0$ si $y\neq x^{\star}$. Nous avons $prox_f(x^{\star}) = x^{\star}$.
\paragraph{$\Leftarrow$} Utilisons la caractérisation de minimisation donnée par le sous - différentiel. Le point $\tilde{x}$ minimise $f(x) +\frac{1}{2}\|x-v\|^2$, c'est à dire $\tilde{x} = prox_f(v)$, si et seulement si 
$$
0\in \partial f(\tilde{x}) + (\tilde{x}-v)
$$
Ou la somme est entendu au sens de la somme entre un ensemble et un singleton. \\
En prenant $x^{\star} = v=\tilde{x}$, nous obtenons $0\in\partial f(x^{\star})$. Et donc $x^{\star}$ minimise $f$.
\end{preuve}

Cela suggère des algorithmes de point fixes. Cela fonctionnerait si $prox_f$ était une contraction (i.e. c-lipschitzienne, $0<c<1$). Ce n'est pas le cas, cependant nous avons une propriété de non expansivité qui est suffisante pour une itération de point fixe : 
$$
\|prox_f(x)-prox_f(y)\|^2\leq \langle prox_f(x)-prox_f(y),x-y\rangle
$$
C'est un cas particulier d'opérateur non expansif (comme par exemple, $-Id$ ou les rotations, \cite{browder1965nonexpansive,brezis1973ope}). 
\begin{definition}{Application non expansive}
Une application $N:\ \RR^n\rightarrow \RR^n$ est dite non expansive si 
$$
\forall (x,y)\in\RR^n\times\RR^n,\quad \|N(x)-N(y)\| \leq \|x-y\|
$$
C'est à dire que $N$ est 1-lipschitzienne. Nous avons la caractérisation suivante : 
$$
\forall (x,y)\in\RR^n\times\RR^n,\quad \|N(x)-N(y)\|^2 \leq  \langle N(x)-N(y),x-y\rangle
$$
\end{definition}

Si $N$ est un opérateur non expansif alors $\forall \alpha \in ]0,1[$ l'opérateur $T = (1-\alpha)I + \alpha N$ a les mêmes poinst fixes que $N$, et qu'en itérant $T$, il y a convergence vers les poinst fixes. C'est à dire, la suite 
$$
x_{n+1} = (1-\alpha )x_n +N(x_n)
$$
converge vers un point fixe de $N$.





 
Soit le problème de minimisation: \\

\fbox{
  \parbox{\textwidth}{
  Soient $f_1$ et $f_2$ deux fonctions de $\Gamma_0(\RR^n)$ telles que $f_1+f_2\in\Gamma_0(\RR^n)$, en particulier le support de $f_1+f_2$ est non vide. Supposons que
  $$
  \lim_{\|x\|\rightarrow+\infty} f_1(x)+f_2(x) =+\infty
  $$
Considérons alors le problème de minimisation, 
Trouver $x\in\RR^n$ qui réalise le 
\begin{align}
\min_{x\in\RR^n}f_1(x)+f_2(x)
\label{eq:pbmminimi}
\end{align}
  }
}

\begin{propriete}
Le problème \ref{eq:pbmminimi} admet une unique solution et pour $\forall \gamma >0$, elle est caractérisée par :
$$
\left\{
\begin{array}{rcl}
x &= & prox_{\gamma f_2} y \\
prox_{\gamma f_2}y &= & prox_{\gamma f_1}(2prox_{\gamma f_2}y -y)
\end{array}
\right.
$$
\end{propriete}

\begin{preuve}
$f_1+f_2$ est une fonction convexe, infinie à l'infini donc l'existence est l'unicité du minimiseur est immédiate.
\end{preuve}

Suivant les idées d'algorithmes de point fixes sur les opérateurs proximaux, nous aintroduisons l'algorithme suivant dit de \emph{Douglas - Rachford}.
\begin{algorithm}
\caption{Douglas-Rachford}
\begin{algorithmic}
\STATE Soit $\epsilon \in ]0,1[,\ \gamma >0,\ y_0\in \RR^n$
\FOR{$n=0,\ 1,\ \ldots$}
\STATE $x_n = prox_{\gamma f_2} y_n$
\STATE $\lambda_n \in [\epsilon,2-\epsilon]$
\STATE $y_{n+1} = y_n + \lambda_n(prox_{\gamma f_1} (2x_n-y_n)-x_n)$
\ENDFOR
\end{algorithmic}
\end{algorithm}

\begin{propriete}
Toute suite $(x_n)$ générée par l'algorithme de Douglas - Rachford converge vers une solution du problème \ref{eq:pbmminimi}.
\end{propriete}








\newpage
\section{Pbm de transport optimal}
Soient deux densités $f_0$ et $f_1$ de même masse, suffisamment régulière, sur un domaine $[0,1]^d$.\\
Un transport de $f_0$ sur $f_1$ est une application $T$ telle que :
$$
f_0(x)=f_1(T(x))|det(\partial T(x))|
$$
Le transport optimal est celui qui minimise le coût $\int c(x,T(x)dx$ parmi tous les transports de $f_0$ sur $f_1$. Dans notre problème : $C(x,y) =\|x-y\|^2$. 

\section{Formulation de Benamou et Brenier}
On montre que la géodésique entre $f_0$ et $f_1$ est : 
$$
f(x,t)=f_0((1-t)Id+tT(x))|det((1-t)Id+t\partial T(x))|
$$
et que ce chemin minimise le problème suivant : 
$$
\min_{(f,v)\in C_v} \int_{[0,1]^d}\int_0^1 f(x,t)\|v(x,t)\|^2dtdx
$$

Où : $C_v= \{(f,v)|\partial_t f+div_x(v)=0;v(0,.)=0,v(1,.)=0,f(.,0)=f_0,f(.,1)=f_1\}$

On pose $m = fv$ pour obtenir le problème de minimisation 
$$
\min_{(f,m)\in C}J(m,f)=\int_{[0,1]^d}\int_0^1 \theta (m,f)dtdx
$$
avec $\theta (m,f) = \frac{\|m\|^2}{f} si f>0 \quad 0 si (m,f)=(0,0)\quad \infty sinon$

Rq : on a ramené le pbm de transport optimal à celui de trouver la géodésique entre les deux densités. 
\section{Méthodes numériques}
On présente le cas 1D qui nous a accaparé. \\
Carré espace temps : $[0,1]^2$ que l'on discrétise : 
$$
G_c = \{(x_i=\frac{i}{N},t_j=\frac{j}{P})|0\leq i\leq N,\ 0\leq j\leq P\}
$$
On notre $E_c =(\RR^2)^{G_c}$ l'espace des variables centrées et $ V=(m_{ij},f_{ij}) \quad 0\leq i\leq N,\ 0\leq j\leq P$ les variables discrétisées. \\

Dans le but de capturer l'équation de continuité, on introduit une grille décentrée : 
$$
G_s^x = \{(x_i=\frac{i+1/2}{N},t_j=\frac{j}{P})|-1\leq i\leq N,\ 0\leq j\leq P\}
$$
et 
$$
G_s^t = \{(x_i=\frac{i}{N},t_j=\frac{j+1/2}{P})|0\leq i\leq N,\ -1\leq j\leq P\}
$$

On note $E_s=\RR^{G_s^x}\times\RR^{G_s^t}$ l'espace des variables décentrées, et $U=(\bar{m_{ij}}\quad 1\leq i\leq N,\ 0\leq j\leq P,\bar{f_{ij}},\quad 0\leq i\leq N,\ -1\leq j\leq P$


\section{Opérateurs}
On introduit plusieurs opérateurs pour lier les variables décentrées et les variables centrées.
\subsection{Interpolation}
$$
I:E_s \rightarrow E_c
$$
tq 
$$
m_{ij} = = (\bar{m}_{i+1/2,j}+\bar{m}_{i-1/2,j})/2.
$$
et
$$
f_{ij} = = (\bar{f}_{i,j+1/2}+\bar{f}_{i,j-1/2})/2.
$$
Cet opérateur s'interprète matriciellement : 
$$
m = \bar{m}I_m
$$
et 
$$
f = I_f\bar{f}
$$

\subsection{Divergence}
l'opérateur qui approxime la divergence
$$
div : E_s\rightarrow \RR^{G_c}
$$
et 
$$
div(\bar{m},\bar{f})_{ij} = (\bar{m}_{i+1/2,j}-\bar{m}_{i-1/2,j}) + (\bar{f}_{i,j+1/2}-\bar{f}_{i,j-1/2})
$$

\subsection{Frontières}
Un opérateur pour extraire les frontières : 

$$
b(\bar{m},\bar{f}) = ((\bar{m}_{-1/2,j},\bar{m}_{N+1/2,j});(\bar{f}_{i,-1/2},\bar{f}_{i,P+1/2}))
$$
et on impose les conditions aux frontières suivantes : 
$$
b(\bar{m},\bar{f}) = b_0=(0,0,f_0,f_1);
$$

\subsection{Problème discrétisé}

On a le problème suivant 

$$
\min_{U=(\bar{m},\bar{f})\in E_s} \theta(I(\bar{m},\bar{f})) + \iota_C(U)
$$
avec l'ensemble des contraintes :
$$
C=\{(\bar{m},\bar{f})\in E_s|\ div(\bar{m},\bar{f}) = 0\ b(\bar{m},\bar{f}) = b_0 \}
$$

\section{Résolution par algorithme de séparation de proximité}
Ces algorithmes sont des généralisation des algos de gradient conjugués. \\

\textbf{Remarque : } On a $J(m,f)=\int_{[0,1]^d}\int_0^1 \frac{\|m\|^2}{f} dtdx$ donc si $f\rightarrow\infty$ on a $J\rightarrow 0$ donc ce n'est pas coercif ce qui rend l'existence de minimiseurs non triviale. Et si $f\rightarrow 0$, on a $j\rightarrow \infty$ donc les méthodes de gradient conjugués ne peuvent pas s'appliquer, le gradient n'est pas lipschitz. \\

On veut résoudre le pbm suivant : 
$$
\min_{z=(U,V)\in E_s\times E_c} G_1(z)+G_2(z)
$$
où, $G_1(z) = J(U)+\iota_C(U)$ et $G_2(z)=\iota_{C_s}(z)$ et $C_s=\{z=(U,V)\in E_s\times E_c\ | \ V=I(U) \}$


\remarque{$G_1$ est la fonctionnelle originelle et $G_2$ vient de notre introduction des variables décalées. }\\

On va alors calculer les opérateurs de proximités de $G_1$ et $G_2$.On dit que $G_1$ est \textbf{simple} dans la mesure ou : 

$$
Prox_{\gamma G_1} (U,V) = (Prox_{\gamma C} (U),Prox_{\gamma J} (V))
$$

\subsection{Opérateur de proximité de $G_2$}

$$
Prox_{\gamma C_s} (U,V) = arg\min_{z'\in C_s} \frac{1}{2}\|z-z'\|^2= Proj_{C_s}(U,V)
$$
Suivant l'article de Papadakis, Peyré et Oudet on a :

$$
Prox_{\gamma C_s} (U,V)  = (\tilde{U},I(\tilde{U}))
$$
et 

$$
\tilde{U} = (Id +I^{\star}I)^{-1}(U+I^{\star}(V))
$$
Ou $I^{\star}$ est l'adjointe de $I$. ce qui nous donne en terme de matrice : 
$$
\tilde{\bar{m}} = (\bar{m}+mI_m^{\star})(Id +I_mI_m^{\star})^{-1}
$$
et 
$$
\tilde{\bar{f}} = (Id +I_f^{\star}I_f)^{-1}(\bar{f}+I_f^{\star}f)
$$
enfin : $\tilde{m}=\tilde{\bar{m}}I_m$ et $\tilde{f}=I_f\tilde{\bar{f}}$

Comme l'opérateur d'interpolation s'interprète matriciellement, son adjoint est donné par la transposée. 

\subsection{Opérateur de proximité de $J$}

$Prox_{\gamma J}(V)=(Prox_{\gamma J}(V_k))_{k\in G_c}$ et 

$$
Prox_{\gamma J}(m_k,f_k) = (\frac{f^{\star}_km_k}{f^{\star}_k+2\gamma},f^{\star}_k)\ si\ f^{\star}>0 \qquad (0,0)\ sinon
$$ 
et $f^{\star}_k$ est la plus grande racine réelle du polynôme de degré 3 donné par : 
$$
P(x) = (X-f_k)(X+2\gamma)^2-\gamma \|m_k\|^2
$$
La racine est calculée rapidement avec l’algorithme de Newton. 

\subsection{Opérateur de proximité de $\iota_C$}

$$
Prox_{\gamma C} = Proj_C
$$
on réécrit l'ensemble des contraintes sous la forme : 

$$
C=\{U\in E_s\ | \ AU=y\} \qquad A=(div,b)^T \qquad y = (0,b0)^T
$$

et on obtient $Proj_C(U)=(Id +A^{\star}\Delta^{-1}A)U + A^{\star} \Delta^{-1}y$ avec $\Delta^{-1}=(A^{\star}A)^{-1}$ et son problème requiert la résolution d'un problème de poisson. 

\section{Algorithme de Douglas Rachford}
On procède à l'itération suivante : $(z^l,w^l)\in (E_s\times E_c)\times (E_s\times E_c)$ avec un $w^0$ donné : 
$$
z^{l+1}=Prox_{\gamma G_2} (w^l)
$$
et 
$$
w^{l+1}=(1-\frac{\alpha}{2})w^l + \frac{\alpha}{2}RProx_{\gamma G_2}\circ RProx_{\gamma G_1} (w^l)
$$

Où on a posé : $RProx_{\gamma G} = 2Prox_{\gamma G}-Id$.\\
On montre alors que pour $\alpha\in ]0,2[$ et $\gamma>0$ la méthode converge $z^l\rightarrow z^{\star}$ ce qui permet de retrouver la géodésique, car : 
$$
z^l=(U^l,V^l)\rightarrow (U^{\star},V^{\star}) \qquad V^{\star}=(m^{\star},f^{\star})
$$


\bibliographystyle{plain}
\bibliography{biblio.bib}
\appendix


\newpage
\addcontentsline{toc}{section}{\listfigurename}
\listoffigures
\addcontentsline{toc}{section}{\lstlistlistingname}
\lstlistoflistings


\end{document}
